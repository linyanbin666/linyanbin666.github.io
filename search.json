[{"categories":["学习笔记"],"content":" 本文是个人学习书籍《一本书讲透Elasticsearch：原理、进阶和工程实践》过程中所记录的一些笔记，内容来源于书籍\nElasticsearch集群 冷热集群架构\n热节点存储用户最为关注的数据，而温或冷节点则用于存放用户不太重要或优先级较低的数据\n可以提高查询性能、降低存储成本、优化集群性能、更好地管理数据和提高可扩展性\n索引生命周期管理\nRollover：滚动索引\n创建符合正则表达式规范（即中间是“-”字符并且后面是数字字符）的索引，并批量导入数据\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 PUT my-index-20240528-000001 { \"aliases\": { \"my-alias\": { \"is_write_index\": true } } } PUT my-alias/_bulk {\"index\": {\"_id\": 1}} {\"title\": \"testing 01\"} {\"index\": {\"_id\": 2}} {\"title\": \"testing 02\"} {\"index\": {\"_id\": 3}} {\"title\": \"testing 03\"} {\"index\": {\"_id\": 4}} {\"title\": \"testing 04\"} {\"index\": {\"_id\": 5}} {\"title\": \"testing 05\"} 基于滚动条件配置实现索引的滚动\nmax_age：在索引层面，指从索引创建开始所经过的时间\nmax_docs：在索引层面，指最大文档数（不含副本分片中的文档）\nmax_size：在索引层面，指索引中所有分片的总存储空间（不含副本）\nmax_primary_shard_size：在分片层面，指索引中最大主分片的存储空间\nmax_primary_shard_docs：在分片层面，指索引中最大主分片的文档数\n1 2 3 4 5 6 7 8 POST my-alias/_rollover { \"conditions\": { \"max_age\": \"7d\", \"max_docs\": 5, \"max_size\": \"5gb\" } } Shrink：压缩索引\n3个必要条件\n副本数为0（”index.number_of_replicas”: 0）\n将分片数据都集中到一个独立的节点（”index.routing.allocation.include._tier_preference”: “data_hot”）\n索引数据只读（”index.blocks.write“: true）\n索引生命周期\n热(Hot)阶段\n温(Warm)阶段\n冷(Cold)阶段\n归档(Frozen)阶段\n删除(Delete)阶段\nDSL命令行实战步骤\n创建生命周期policy\n创建索引模板，在模板中关联policy和别名\n创建符合模板的起始索引，并插入数据\n索引基于配置的ILM滚动\n验证结果是否达到预期\n跨机房、跨机架部署\n设置节点属性\n1 2 3 4 # 节点1设置 node.attr.rack_id: rack_01 # 节点2设置 node.attr.rack_id: rack_02 设置集群层面的分片分配策略\n1 2 3 4 5 6 7 PUT _cluster/settings { \"persisent\": { \"cluster.routing.allocation.awareness.attributes\": \"rack_id\", \"cluster.routing.allocation.awareness.force.rack_id.values\": \"rack_01,rack_02\" } } 索引/集群的备份与恢复\n常见方案\n使用Elasticsearch的快照和恢复功能进行备份和恢复。该方案适用于集群整体备份与迁移，包括全量、增量备份和恢复\n通过reindex操作在集群内或跨集群同步数据。该方案适用于相同集群但不同索引层面的迁移，或者跨集群的索引迁移\n使用elasticdump来迁移映射和数据。该方案适用于仅对索引层面进行数据或映射的迁移\nElasticsearch快照和恢复功能\n快照执行步骤\n配置快照存储路径\n1 path.repo: [\"path\"] 注册快照存储库\n1 2 3 4 5 6 7 PUT /_snapshot/my_backup { \"type\": \"fs\", \"settings\": { \"location\": \"path\" } } 拍摄快照\n1 2 3 4 5 6 7 8 9 10 11 12 13 # 集群快照 PUT /_snapshot/my_backup/snapshot_cluster?wait_for_completion=true # 索引快照 PUT /_snapshot/my_backup/snapshot_hamlet_index?wait_for_completion=true { \"indices\": \"hamlet_*\", \"ignore_unavailable\": true, \"include_global_state\": false, \"metadata\": { \"taken_by\": \"horin\", \"taken_because\": \"backup before restart\" } } 恢复索引快照\n1 POST /_snapshot/my_backup/snapshot_hamlet_index/_restore 快照常见操作\n查看所有快照库\n1 GET /_snapshot/all 查看快照状态\n1 GET /_snapshot/my_backup/snapshot_hamlet_index/_status 删除快照\n1 DELETE /_snapshot/my_backup/snapshot_hamlet_index 应用场景\n数据备份\n集群迁移\n复制生产数据\n数据恢复\n高可用性\n快照生命周期管理\n跨集群检索\nElasticsearch安全 从6.8版本（尤其是7.1及以上版本）开始，Elasticsearch的基础级别安全功能永久免费，在Elasticsearch 8.X版本之后，安全功能不再是可选项，而是必须项\nElasticsearch的安全机制\n认证和授权：Elasticsearch支持基于用户名和密码的认证及授权机制\n传输加密：Elasticsearch可以使用TLS（传输层安全）协议来加密网络传输\n数据加密：Elasticsearch支持字段级别的加密（付费功能）\n安全插件：Elasticsearch提供了一些安全插件（付费功能）\n安全使用Elasticsearch脚本\n脚本类型细分\nstored类型脚本：stored类型的脚本是先定义好脚本，将其存储起来，后续既可以使用，也可以不使用\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # 定义脚本 POST _scripts/sum_score_script { \"script\": { \"lang\": \"painless\", \"source\": \"ctx._source.total_score = ctx._source.math_score + ctx._source.english_score\" } } # 使用脚本 POST my_index/_update_by_query { \"script\": { \"id\": \"sum_score_script\" }, \"query\": { \"match_all\": {} } } inline类型脚本：inline类型脚本就是在使用时直接指定脚本，而不提前创建脚本\n1 2 3 4 5 6 7 8 9 10 POST my_index/_update_by_query { \"script\": { \"lang\": \"painless\", \"source\": \"ctx._source.total_score = ctx._source.math_score + ctx._source.english_score\" }, \"query\": { \"match_all\": {} } } 脚本限制配置（在elasticsearch.yml中设置）\n脚本分级限制（script.allowed_types）：默认配置为both，即不做任何限制；将配置修改为inline或stored则表示限制只能执行指定类型的脚本；将配置修改为none，则表示完全禁止执行脚本\n脚本可用范围（script.allowed_contexts）：默认为none，即不做任务限制；可以配置下选项有下面几种，如果需要同时允许多种选项，则可以用英文逗号进行分隔\nscoring：计算评分\nupdate：更新\ningest processor：管道预处理\nreindex：索引迁移\nsort：排序\nmetric aggregation map：指标聚合\n……\nElasticsearch运维 Elasticsearch集群监控的维度及指标\n5个重要监控维度\n集群健康维度：分片和节点\n通过GET _cluster/health监视集群\nstatus：集群的状态，其中“红色”是指部分主分片未分配成功，“黄色”是指部分副本分片未分配成功，“绿色”是指所有分片均分配成功\nactive_primary_shards：集群中活动主分片的数量\nrelocating_shards：重定位分片数，指由于节点丢失而移动的分片计数\ninitializing_shards：初始化分片数，指由于添加索引而初始化的分片计数\nunassigned_shards：未分配的分片数\n搜索性能维度：请求率和延迟\n通过GET index_a/_stats查看对应目标索引状态\nquery_current：集群当前正在处理的查询操作的计数\nfetch_current：集群中正在进行的取回操作的计数\nquery_total：集群处理的所有查询的总数\nquery_time_in_millis：所有查询消耗的总时间\nfetch_total：集群处理的所有取回操作的总数\nfetch_time_in_millis：所有取回操作消耗的总时间\n索引性能维度：刷新和合并时间\n通过GET _nodes/stats获取索引性能指标\nrefresh.total：总刷新计数\nrefresh.total_time_in_millis：刷新总时间\nmerges.current_docs：当前的合并文档数\nmerges.total_docs：合并的总文档数\nmerges.total_stopped_time_in_millis：合并操作所消耗的总时间\n节点运行状况维度：内存、磁盘和CPU指标\n通过GET _cat/nodes?v\u0026h=id,disk.total,disk.used,disk.avail,disk.used_percent,ram.current,ram.percent,ram.max,cpu获取节点运行指标\ndisk.total：存储容量\ndisk.used：存储空间使用量\ndisk.avail：可用存储空间总量\ndisk.used_percent：已使用的存储空间的百分比\nram.current：当前内存使用量\nram.percent：当前内存使用百分比\nram.max：内存总量\ncpu：CPU使用百分比\nJVM运行状况维度：GC和堆内存使用率\n通过GET /_nodes/stats命令检索JVM度量标准\njvm.mem：内存使用情况\njvm.threads：当前使用的线程数和最大线程数\njvm.gc：垃圾收集相关指标\n集群故障排查及修复指南\n如何定位红色或黄色的索引\n确定我们所能知道的主要问题\n确定哪些索引有问题，多少索引有问题\n1 GET _cat/indices?v\u0026health=yellow 查看有问题的分片及其原因\nv：显示表头\nh：指定要在结果中显示的列\ns：指定对结果进行排序的列\n1 GET _cat/shards?v\u0026h=n,index,shard,prirep,state,sto,sc,unassigned.reason,unassigned.details\u0026s=sto,index 进一步定位未分配的原因\n1 2 3 4 5 6 GET _cluster/allocation/explain { \"index\": \"my_index\", \"shard\": 0, \"primary\": false } 对症下药，解决问题\n运维及故障诊断的常用命令\n集群节点下线：保证集群颜色绿色的前提下，使某个节点优雅下线\n1 2 3 4 5 6 PUT _cluster/settings { \"transient\": { \"cluster.routing.allocation.exclude_ip\": \"127.0.0.1\" } } 强制刷新：刷新索引可以确保当前仅存储在事务日志中的所有数据也永久存储在Lucene索引中\n1 POST _flush 更改并发分片的数量以平衡集群：控制在集群范围内有多少并发分片，以达到重新平衡，默认值为2\n1 2 3 4 5 6 PUT _cluster/settings { \"transient\": { \"cluster.routing.allocation.cluster_concurrent_rebalance\": 2 } } 更改每个节点同时恢复的分片数量：如果节点已从集群断开连接，则其所有分片都将变为未分配状态。经过一定的延迟后，分片将被分配到其他位置。每个节点要恢复的并发分片数由该设置确定\n1 2 3 4 5 6 PUT _cluster/settings { \"transient\": { \"cluster.routing.allocation.node_concurrent_recoveries\": 6 } } 调整恢复速度：为了避免集群过载，Elasticsearch限制了恢复速度\n1 2 3 4 5 6 PUT _cluster/settings { \"transient\": { \"indices.recovery.max_byte_per_sec\": \"80mb\" } } 清除节点上的缓存：如果节点达到较高的堆内存值，则可以在节点级别上调用如下API以使Elasticsearch清理缓存。这会降低性能，但可以摆脱内存不足引发的OOM的困扰\n1 POST _cache/clear 调整断路器：为了避免Elasticsearch OOM，可以调整断路器上的设置。这将限制搜索所占用的内存，并舍弃所有估计消耗内存会超出阈值的搜索请求\n1 2 3 4 5 6 PUT _cluster/settings { \"transient\": { \"indices.breaker.total.limit\": \"40%\" } } 集群迁移：集群数据迁移、索引数据迁移等\n方案一：针对索引部分或者全部数据执行reindex指令\n借助第三方工具（如elasticdump和elasticsearch-migration）迁移索引或者集群，本质上是一种“scroll+bulk”方案的实现\n集群数据备份和恢复：对于高可用业务场景，定期增量、全量数据备份\n1 2 3 4 5 6 7 8 9 10 11 12 13 # 索引快照 PUT /_snapshot/my_backup/snapshot_hamlet_index?wait_for_completion=true { \"indices\": \"hamlet_*\", \"ignore_unavailable\": true, \"include_global_state\": false, \"metadata\": { \"taken_by\": \"horin\", \"taken_because\": \"backup before restart\" } } # 恢复索引 POST /_snapshot/my_backup/snapshot_hamlet_index/_restore Elasticsearch日志\n作用是什么？进行集群状态监测和故障诊断\n默认路径是什么？$ES_HOME/logs\n基于什么组件实现的？Log4j2\n配置文件是什么？log4j2.properties\n配置内容主要是什么？命名规范、日志随日期滚动策略（日志大小等条件设置）等\n日志的级别如何划分？TRACE→DEBUG→INFO→WARN→ERROR→FATAL\n日志级别调整方式有什么？\n方式一：支持动态更新，无须重启\n1 2 3 4 5 6 PUT _cluster/settings { \"persistent\": { \"logger.org.elasticsearch.discovery\": \"DEBUG\" } } 方式二：elasticsearch.yml配置（静态配置方式，重启后生效）\n1 logger.org.elasticsearch.discovery: DEBUG 方式三：log4j2.properties配置（静态配置方式，重启后生效）\n1 2 logger.discovery.name = org.elasticsearch.discovery logger.discovery.level = DEBUG slowlog：指慢日志，又可以分为慢检索日志和慢写入日志\nElasticsearch核心工作原理 Elasticsearch文档版本冲突（本质：老版本覆盖掉了新版本）\n创建场景的版本冲突\n1 2 3 4 5 # 重复执行create会出现冲突 POST my_index_1501/_create/1 { \"title\": \"hello\" } 批量更新场景的版本冲突\n批量删除场景的版本冲突\n如何解决或避免Elasticsearch文档版本冲突\n利用external对版本号进行外部控制\n1 2 3 4 POST my_index_1501/_doc/1?version=2\u0026version_type=external { \"title\": \"hello2\" } 利用if_seq_no和if_primary_term作为唯一标识来避免版本冲突\n1 2 3 4 5 6 7 8 # 查看_seq_no和_primary_term GET my_index_1501/_doc/1 # 基于获取到的_seq_no和_primary_term进行更新 POST my_index_1501/_doc/1?if_seq_no=2\u0026if_primary_term=1 { \"title\": \"hello2\" } 批量更新和批量删除中通过proceed忽略冲突\n1 2 3 4 POST my_index_1501/_update_by_query?conflicts=proceed { ... } Elasticsearch文档更新/删除的原理\n更新/删除操作时文档版本号的变化\n更新、删除操作实际是在原来文档的版本号基础上加1，且每执行一次，版本号进行一次加1操作\n原来老版本的文档被标记为“已删除”状态\n文档删除、索引删除和文档更新的本质\n文档删除的本质：本质上是逻辑删除而非物理删除，文档被删除时，文档版本+1且标识为“已删除”，等到段合并时才会从磁盘上删除这些文档\n1 2 # 强制执行段合并，only_expunge_deletes**表示**只清除已标记为“已删除”的文档 POST my_index/_forcemerge?only_expunge_deletes 索引删除的本质：本质上是物理删除，索引删除包含两个步骤：更新集群、将分片从磁盘删除\n文档更新的本质：本质上是“delete+add”操作，将旧文档标记为“已删除”，并增加一个全新的文档\nElasticsearch写入的原理\n核心概念\n分段：在Elasticsearch中，段是由一些倒排索引和一些元数据组成的。分段是将索引分成多个小段的过程，每个分段包含一部分索引数据\n1 GET my_index/_segments 事务日志文件：一个文档被索引之后，Elasticsearch就会将其添加到内存缓冲区，并且同时写入事务日志文件(translog)中。translog的作用是保证数据的可靠性和一致性\n倒排索引是不可变的：已写入磁盘的倒排索引永远不会改变\n段是不可变的：段一旦被创建，就不能被修改，任何对索引的更改都会生成新的段\n写入的实现流程\n客户端向主节点发送写数据请求，主节点充当协调节点的角色\n主节点使用文档ID确认文档所属分片，将请求转发到主分片所属的节点\n在主分片上面执行写入操作，如果写入成功，将请求并行转发到副本分片上，并向协调节点报告写入成功\n协调节点向客户端报告写入结果\nrefresh和flush操作\nrefresh操作\n将文档插入Elasticsearch时，文档会被写入内存缓冲区中，然后通过refresh（刷新）操作定期从该缓冲区刷新到内存段中。刷新频率由refresh_interval参数控制，默认1s。\n1 2 3 4 5 6 7 8 PUT my_index { \"settings\": { \"index\": { \"refresh_interval\": \"60s\" } } } refresh操作在本质上是将写入数据由内存缓冲区写入内存段中，以保证搜索可见\nflush操作\n当新的文档写入后，写入索引缓冲区(index buffer)的同时会写入事务日志，当flush操作后事务日志才可清空\nflush操作将文件系统缓存(filesystem cache)写入磁盘，以达到持久化的目的\nElasticsearch段合并的原理\n段的基础知识\n一个集群包含一个或多个节点\n一个节点包含一个或多个索引\n一个索引由一个或多个分片组成\n一个分片对应一个Lucene索引\n一个分片包含多个段，每个段是一个倒排索引，查询时会把所有段的查询结果汇总，并将其作为最终分片的查询结果，每个段时磁盘中的单个文件\n段合并\n将一些大小相似的段合并成更大的段，段合并的时候会将那些旧的已删除文档从文件系统中清除\n作用\n提高搜索效率：合并后的大段可以减少查询时需要扫描的段的数量，从而提高搜索效率\n释放空间：合并后的段可以减少占用的磁盘空间，从而释放空间，减少硬盘的IO开销\n优化索引结构：段合并后可以优化索引结构，减少冗余数据，从而进一步提高搜索效率\n潜在问题\n资源消耗率高：段合并操作需要占用系统资源，例如CPU、内存、磁盘资源等\n磁盘碎片增多：段合并操作可能导致磁盘碎片，因为合并后的段可能不是连续的，而是由多个不连续的片段组成的\n写入或检索延迟大：如果进行段合并操作时需要合并的段数量过多，可能会导致合并操作的时间较长，从而延迟写入操作和搜索操作\n极端情况下索引不可用：如果段合并操作失败或被中断，则可能会导致索引不可用，需要进行恢复操作\n段合并问题的优化建议\n在非业务密集时间段实施段合并操作\n如果对数据的实时性要求并不严格，建议将refresh_interval参数设置为30s或更长\n根据CPU核心数量调整index.merge.scheduler.max_thread_count参数\nElasticsearch检索的原理\n核心步骤\n客户端发起请求\n在主节点或协调节点中，需要验证查询主体\n选择要在查询中使用的索引，根据路由机制选择待检索的分片（主分片或者副本分片）\n在数据节点中执行查询操作，以收集精准匹配结果或者满足条件的结果\n接收第四步的结果，在协调节点做结果整合，并计算相关性评分\n将结果返回给用户\nElasticsearch性能优化 Elasticsearch性能指标\n硬件性能指标：包括CPU使用率、内存使用率、磁盘IO读写速度、网络带宽等\nElasticsearch内部指标：包括响应时间、吞吐量、并发数、负载等\n响应时间：指从客户端发出请求到Elasticsearch返回结果所需的时间\n吞吐量：指在一段时间内Elasticsearch处理的请求数量\n并发数：指同时连接到Elasticsearch的客户端数量\n负载：指Elasticsearch系统中正在处理的请求数量，包括正在进行的搜索、索引、聚合和删除数据等操作\n网络指标：包括网络延迟、带宽、吞吐量等\nElasticsearch通用的性能优化建议\n版本选型：考虑当下，兼顾未来\n功能要求\n稳定性和性能\n兼容性\n安全性\n确保集群健康状态\n硬件资源匹配到位\n足够内存\nSSD\n多核CPU\n合理进行集群配置\n各节点尽量不要和其他业务功能共用一台机器\n若集群节点数小于或者等于3个，则建议采用默认节点角色；若集群节点数多于3个，则建议根据业务场景需要逐步独立出主节点角色、数据节点角色和协调节点角色\n调整分片和副本数\n调整索引刷新频率\n合理使用缓存\n集群安全为第一要务\n防火墙\n认证授权\nTLS/SSL加密\n安全补丁更新\n务必提前做好集群监控\n监控硬件资源\n监控Elasticsearch集群状态\n监控日志\n监控性能指标\n监控异常情况\n用Elasticsearch处理匹配场景下的合理需求\n文本搜索和聚合\n倒排索引\n分布式架构\n写入前预处理\nElasticsearch写入优化\n写入优化建议\n写入时先将副本分片数置为0，完成写入后再将其复原\n优先使用系统自动生成ID的方式（手动指定ID的方式会多一次查询）\n合理调整刷新频率\n合理调整堆内存中的索引缓冲区大小（index.index_buffer_size，默认值为堆内存的10%）\n给堆外的内存留够空间\n批量写入而非单个文档写入\n多线程并发写入\n合理设置线程池和队列大小\n设置合理的映射\n合理使用分词器\n必要时使用SSD\n合理设置集群节点角色\n推荐使用官方客户端的API\n写入过程监控\n索引率：索引率是指每秒写入Elasticsearch的文档数，是衡量集群写入性能的关键指标\n查询速率：查询速率表示每秒的查询次数，反映了Elasticsearch集群的读取性能\nElasticsearch检索优化\n全量数据和大文档处理的优化建议\n不要返回全量或近全量数据\n避免使用大文档\n数据建模层面的优化建议\n文档结构务必规范、一致\n设置合理的分片数和副本数\n主分片一般建议设置为数据节点的1～3倍\n对于一般的非高可用场景，一个副本基本足够\n多使用写入前预处理操作\n合理使用边写入边排序机制\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # 创建索引时指定排序顺序 PUT my_index { \"settings\": { \"index\": { \"sort.field\": \"date\", \"sort.order\": \"desc\" } }, \"mappings\": { \"properties\": { \"date\": { \"type\": \"date\" } } } } 多表关联按需选型\n检索方法层面的优化建议\n尽可能减少检索字段数目\n合理设置size值\n尽量使用keyword字段类型\n尽量避免使用脚本\n有效使用filter缓存\n对历史索引数据不定期进行段合并（不要对正在写入数据的索引进行段合并）\n预热文件系统缓存\n1 2 3 4 5 6 PUT my_index { \"settings\": { \"index.store.preload\": [\"nvd\", \"dvd\"] } } 通过perference优化缓存利用率\n避免使用wildcard检索（推荐在前期使用预处理Ngram分词，以空间换时间来解决问题）\n尽量避免使用正则匹配检索\n谨慎使用全量聚合和多重嵌套聚合\n性能优化的DSL命令行\n未分配分片查看\n1 GET _cat/shards?v\u0026h=index,shard,prirep,state,unassigned.reason\u0026s=state:asc 动态调整副本数\n1 2 3 4 PUT my_index/_settings { \"number_of_replicas\": 0 } 重新打开分片分配策略\n1 2 3 4 5 # 开启Elasticsearch集群的分片分配策略，允许分片在节点间重新分配 PUT _cluster/settings { \"cluster.routing.allocation.enable\": \"all\" } 手动移动未分配的分片\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 POST _cluster/reroute { \"commands\": [ { \"move\": { \"index\": \"my_index\", \"shard\": 0, \"from_node\": \"node1\", \"to_node\": \"node2\" } }, { \"allocate_replica\": { \"index\": \"my_index\", \"shard\": 1, \"node\": \"node3\" } } ] } 查看磁盘使用率\n1 GET _cat/allocation?v 查看各个节点的版本号\n1 GET _cat/nodes?v\u0026h=host,name,version 检索优化实战\n慢日志设置\n1 2 3 4 5 6 7 8 9 10 11 12 PUT my_index/_settings { \"index.search.slowlog.threshold.query.warn\": \"10s\", \"index.search.slowlog.threshold.query.info\": \"5s\", \"index.search.slowlog.threshold.query.debug\": \"2s\", \"index.search.slowlog.threshold.query.trace\": \"500ms\", \"index.search.slowlog.threshold.fetch.warn\": \"1s\", \"index.search.slowlog.threshold.fetch.info\": \"800ms\", \"index.search.slowlog.threshold.fetch.debug\": \"500ms\", \"index.search.slowlog.threshold.fetch.trace\": \"200ms\", \"index.search.slowlog.level\": \"info\" } 段合并\n1 POST my_index/_forcemerge 写入优化实战\n批量写入\n1 2 3 PUT _bulk {\"index\": {\"_index\": \"my_index\"}} {\"field\": \"value\"} 提高刷新频率\n1 2 3 4 5 6 PUT my_index/_settings { \"index\": { \"refresh_interval\": \"30s\" } } 将副本数设置为0\n1 2 3 4 PUT my_index/_settings { \"number_of_replicas\": 0 } 通过translog命令进行异步刷盘\n1 2 3 4 5 6 7 8 PUT my_index/_settings { \"index\": { \"translog\": { \"durability\": \"async\" } } } Elasticsearch实战“避坑”指南 分片及副本设置建议\n分片数和节点数应该相对平衡\n设置total_shards_per_node，将索引压力分摊至多个节点\n尽量保证每个分片大小一致\n主分片设置参考准则\n设置合理的主分片数目。建议设置为节点数目的1倍到3倍（结合业务场景和集群规模），以确保每个节点都有分片\n避免将主分片数目设置得太高或太低\n监控主分片的状态和性能，使用Elasticsearch监控工具来及时发现和解决问题\n考虑主分片的恢复时间，避免主分片过大导致恢复时间过长，从而影响索引的可靠性\n副本分片设置参考准则\n设置合理的副本分片数目。建议至少设置1个副本\n避免在节点数目较少的情况下设置过多的副本分片\n25个核心Elasticsearch默认值\n参数类型\n集群级别参数\n举例1：cluster.max_shards_per_node，前缀是cluster.*，修改针对集群生效\n举例2：indices.query.bool.max_clause_count，需要在elasticsearch.yml配置文件中设置，重启Elasticsearch后生效\n索引级别参数\n举例：index.number_of_shards，前缀是index.*，修改针对索引生效 区分静态参数和动态参数\n静态参数，如主分片数index.number_of_shards，在索引创建之后就不能更改，除非重建索引\n动态参数，如副本分片数index.number_of_replicas，允许在任何时候进行动态调整，可以通过update API进行操作\n6个Elasticsearch集群级别参数的关键默认值\nboolean类型默认支持的最大子句个数\n对应参数为indices.query.bool.max_clause_count，为静态参数（需要在elasticsearch. yml中设置），默认最大为1024 数据节点支持的默认分片个数\n对应参数为cluster.max_shards_per_node，默认最大为1000（7.X版本后） 堆内存中索引缓冲区的默认比例\n对应参数为indices.memory.index_buffer_size、indices.memory.min_index_buffer_size、indices.memory.max_index_buffer_size，均为静态参数（需要在elasticsearch.yml中设置）。indices.memory.index_buffer_size的默认值为10%，indices.memory.min_index_buffer_size的默认值为48 MB 默认磁盘使用率\n对应参数为cluster.routing.allocation.disk.watermark.low/high/flood_stage，为集群动态参数。cluster.routing.allocation.disk.watermark.low默认为85%，cluster.routing.allocation.disk.watermark.high默认为90%，cluster.routing.allocation.disk.watermark.flood_stage默认为95% 默认GC方式\n对应参数为-XX:+UseConcMarkSweepGC、-XX:CMSInitiatingOccupancyFraction=75、-XX:+UseCMSInitiatingOccupancyOnly 7个Elasticsearch索引级别参数的关键默认值\n默认主分片的大小\n对应参数为index.number_of_shards，为静态参数，默认值为1；单索引支持最大分片数为1024 默认的压缩算法\n对应参数为index.codec，为静态参数，默认值为LZ4 默认副本分片个数\n对应参数为index.number_of_replicas，为动态参数，默认值为1 默认刷新频率\n对应参数为index.refresh_interval，为动态参数，默认最小值为1s terms默认支持最大长度\n对应参数为index.max_terms_count，为动态参数，默认最大值为65536 默认返回的最大搜索结果数\n对应参数为index.max_result_window，为动态参数，默认最大值为10000 默认预处理管道\n对应参数为index.default_pipeline，为动态参数，默认自定义管道 4个Elasticsearch映射级别参数的关键默认值\n默认支持的最大字段数\n对应参数为index.mapping.total_fields.limit，为动态参数，默认最大值为1000 Mapping字段默认的最大深度\n对应参数为index.mapping.depth.limit，为动态参数，默认最大值为20 默认支持的Nested类型个数\n对应参数为index.mapping.nested_fields.limit，表示一个索引所支持的最大Nested类型个数，默认值为50；以及index.mapping.nested_objects.limit，表示一个Nested类型所支持的最大对象数，默认值为10000\n如果子文档频繁更新，则建议使用父子文档。如果子文档不频繁更新但查询频繁，则建议采用Nested类型\n动态映射条件下默认匹配的字符串类型\n字符串类型默认为“text+keyword”类型 8个其他关键默认值\nElasticsearch默认的评分机制\n默认为BM25 Elasticsearch keyword类型默认支持的字符数\nkeyword类型支持的最大长度为32766个UTF-8字符，而text类型对字符长度没有限制。设置ignore_above后，超过给定长度的数据将不被索引，无法通过term检索返回结果 Elasticsearch集群节点默认属性值\nElasticsearch客户端默认请求节点\n如果不明确指定协调节点，则由默认请求的节点充当协调节点的角色。每个节点都是一个隐式的协调节点 Elasticsearch默认分词器\n在不明确指定分词器的场景，默认采用标准分词器（standard） Elasticsearch聚合默认UTC时间\nElasticsearch默认堆内存大小\nElasticsearch 8.X版本的默认堆内存大小是4GB Elasticsearch默认集成JDK\nElasticsearch线程池和队列\n查看线程池全貌\n1 GET _cat/thread_pool?v\u0026h=id,name,active,rejected,completed,size,type\u0026pretty\u0026s=type Elasticsearch根据在每个节点中检测到的线程数（即number of processors）自动配置线程池参。如果检测失败，则应在elasticsearch.yml中显式设置硬件中可用的线程数\n线程池实战问题及注意事项\n修改线程池和队列需要更改配置文件elasticsearch.yml\n集群拒绝请求的原因可能有多种\n写入批量值的递进步长调优\nElasticsearch热点线程\n1 2 GET _nodes/hot_threads GET _nodes/\u003cnode_id\u003e/hot_threads 规划Elasticsearch集群规模和容量（略）\nElasticsearch Java客户端选型（略）\nElasticsearch缓存\n节点查询缓存(node query cache)\n应用场景和缓存条件\n适用于term检索和filter检索\n在Elasticsearch中，对于一个分片内的特定段，只有当该段至少含有10000个文档且段中的文档数量超过了该分片总文档数的3%时，它才会被纳入缓存中\n缓存配置\nindices.queries.cache.size：该配置用于控制filter缓存的堆内存大小，可以是百分比值（例如5%）或精确值（例如512MB），默认为10%，该配置为静态配置，需要在集群的每个数据节点进行配置\nndex.queries.cache.enabled：该配置用于控制是否启用节点查询缓存，只能在创建索引或者关闭索引时进行设置，默认为true，该配置为静态配置\n分片请求缓存(shard request cache)\n应用场景和缓存条件\n适用于日志用例场景，在这种情况下，数据不会在旧索引上更新\n默认情况下，分片请求缓存仅缓存size=0的搜索请求的结果\n缓存配置\nindex.requests.cache.size：该配置用于控制分片缓存的堆内存大小，默认为1%，该配置为静态配置，需要在集群的每个数据节点进行配置 字段缓存\nElasticsearch数据建模\n基于业务建模\n基于数据量建模\n基于设置层面建模\n基于映射层面建模\n字段命名要规范\n字段类型要合理\n分词器要灵活\n多字段类型灵活使用\n基于复杂索引关联建模\n利用JMeter进行Elasticsearch性能测试（略）\n","description":"","tags":["ElasticSearch","ES"],"title":"书籍学习-一本书讲透Elasticsearch：原理、进阶和工程实践（三）","uri":"/posts/2024-08-19-learning_es_1_3/"},{"categories":["学习笔记"],"content":" 本文是个人学习书籍《一本书讲透Elasticsearch：原理、进阶和工程实践》过程中所记录的一些笔记，内容来源于书籍\nElasticsearch文档 新增文档\n单条写入（PUT /_doc/[id]）\n批量写入（POST [index]/_bulk）\n删除文档\n单个删除（DELETE /_doc/）\n批量删除（POST /_delete_by_query）\n修改/更新文档\n前置条件：mapping内的_source字段必须设置为true，这也是默认设置。如果将其手动设置为false，执行update会报错\n单个文档部分更新\n在原有基础上新增字段（POST /_update/，“doc”）\n在原有字段基础上部分修改字段值（POST /_update/，“script”）\n存在则更新，不存在则插入给定值（POST /_update/，“upsert”）\n全部文档更新（PUT /_doc/）\nPS：Mapping不会因为文档的写入或更新操作而导致收缩，除非通过reindex操作将数据迁移到新的索引上\n批量文档更新\n基于Painless脚本的批量更新（POST /_update_by_query，”script”）\n基于ingest预处理管道的批量更新（POST /_update_by_query?pipeline=）\n取消更新\n获取更新任务的任务ID（GET tasks?detailed=true\u0026actions=*by_query）\n查看任务ID，了解任务详情（GET /_tasks/）\n取消更新任务（POST tasks//cancel）\n迁移文档（reindex）\n同集群索引之间的全量数据迁移\n1 2 3 4 5 6 7 8 9 10 POST _reindex { \"conflicts\": \"proceed\", \"source\": { \"index\": \"source_index\" }, \"dest\": { \"index\": \"dest_index\" } } 同集群索引之间基于特定条件的数据迁移\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 POST _reindex { \"source\": { \"index\": \"source_index\", \"query\": { \"term\": { \"user\": \"horin\" } } }, \"dest\": { \"index\": \"dest_index\" } } 基于script脚本的索引迁移\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 POST _reindex { \"source\": { \"index\": \"source_index\" }, \"dest\": { \"index\": \"dest_index\", \"version_type\": \"external\" }, \"script\": { \"lang\": \"painless\", \"source\": \"if (ctx._source.user == 'horin') { ctx._source.remove('user') }\" } } 基于预处理管道的数据迁移\n1 2 3 4 5 6 7 8 9 10 POST _reindex { \"source\": { \"index\": \"source_index\" }, \"dest\": { \"index\": \"dest_index\", \"pipeline\": \"my_pipeline\", } } 不同集群索引之间的数据迁移\nelasticsearch.yml配置文件配置白名单\n1 reindex.remote.whitelist: \"xxx\" 同普通的跨索引数据迁移的实现方式一致\n1 2 3 4 5 6 7 8 9 10 11 12 POST _reindex { \"source\": { \"remote\": { \"host\": \"http://otherhost:9200\" }, \"index\": \"source_index\" }, \"dest\": { \"index\": \"dest_index\" } } 查看及取消reindex任务\n1 2 3 4 5 6 # 获取reindex相关任务 GET _tasks?detailed=true\u0026actions=*reindex # 获取任务ID及相关信息 GET /_tasks/\u003ctaskId\u003e # 取消任务 POST /_tasks/\u003ctaskId\u003e/_cancel 业务零掉线情况下的数据迁移：reindex + 别名\nElasticsearch脚本 应用场景\n自定义字段\n1 2 3 4 5 6 7 8 9 10 11 # 截取返回日期格式中的年份 POST my_index/_search { \"script_fields\": { \"insert_year\": { \"script\": { \"source\": \"doc['insert_year'].value.getYear()\" } } } } 自定义评分\n1 2 3 4 5 6 7 8 9 10 11 12 # 自定义评分检索 POST my_index/_search { \"function_score\": { \"script_score\": { \"script\": { \"lang\": \"expression\", \"source\": \"_score * doc['popularity']\" } } } } 自定义更新\n1 2 3 4 5 6 7 8 9 10 11 12 13 # 已有字段更新为其他字段 POST my_index/_update/1 { \"script\": { \"lang\": \"painless\", \"source\": \"\"\" ctx._source.theatre = params.theatre \"\"\", \"params\": { \"theatre\": \"jingju\" } } } 自定义reindex\n自定义聚合\n1 2 3 4 5 6 7 8 9 10 11 12 13 POST my_index/_search { \"aggs\": { \"terms_aggs\": { \"terms\": { \"script\": { \"lang\": \"painless\", \"source\": \"doc['popularity'].value\" } } } } } 其他自定义操作\nElasticsearch检索 常用检索\n全文检索\nmatch\n应用于高召回率和结果精准度要求较低的场景\n1 2 3 4 5 6 7 8 POST my_index/_search { \"query\": { \"match\": { \"title\": \"乌兰新闻\" } } } match_phrase（短语检索）\n适用于注重精准度的召回场景，match_phrase检索要求查询的词条顺序和文档中的词条顺序保持一致，强调短语的完整性和顺序\n1 2 3 4 5 6 7 8 9 10 POST my_index/_search { \"query\": { \"match_phrase\": { \"title\": { \"query\": \"乌兰新闻\" } } } } match_phrase_prefix（短语前缀检索）\n查询词语需要按顺序匹配文档中的内容，同时允许最后一个词语只匹配其前缀\n1 2 3 4 5 6 7 8 POST my_index/_search { \"query\": { \"match_phrase_prefix\": { \"title\": \"乌兰新\" } } } multi_match（多字段检索）\n适用于在多个字段上执行match检索的场景\n1 2 3 4 5 6 7 8 9 POST my_index/_search { \"query\": { \"multi_match\": { \"query\": \"乌兰\", \"fields\": [\"title^3\", \"message\"] } } } query_string（支持与或非表达式的检索）\n允许用户使用Lucene查询语法直接编写复杂的查询表达式\n1 2 3 4 5 6 7 8 9 POST my_index/_search { \"query\": { \"query_string\": { \"default_field\": \"title\", \"query\": \"乌兰 AND 新闻\" } } } simple_query_string\nsimple_query_string和query_string的区别\nsimple_query_string对语法的核查并不严格\nsimple_query_string是一种简单的查询语法，只支持单词查询、短语查询或者包含查询，不支持使用通配符和正则表达式\nmatch boolean prefix\nintervals\ncombined fields\n精确匹配\nterm（单字段精确匹配）\n应用于单字段精准匹配的场景\nterm检索针对的是非text类型，用于text类型时并不会报错，但检索结果一般会达不到预期\n1 2 3 4 5 6 7 8 9 10 POST my_index/_search { \"query\": { \"term\": { \"name\": { \"value\": \"horin\" } } } } terms（多字段精确匹配）\n应用于多值精准匹配场景，例如筛选出具有多个特定标签或状态的项目\n1 2 3 4 5 6 7 8 POST my_index/_search { \"query\": { \"terms\": { \"tags\": [\"weibo\", \"wechat\"] } } } range（范围检索）\n适合对数字、日期或其他可排序数据类型的字段进行范围筛选\n1 2 3 4 5 6 7 8 9 10 11 POST my_index/_search { \"query\": { \"range\": { \"popular_degree\": { \"gte\": 10, \"lte\": 100 } } } } exists（是否存在检索）\n适用于检查文档中是否存在某个字段，或者该字段是否包含非空值\n1 2 3 4 5 6 7 8 POST my_index/_search { \"query\": { \"exists\": { \"field\": \"title.keyword\" } } } wildcard（通配符检索）\n适用于对部分已知内容的文本字段进行模糊检索\n1 2 3 4 5 6 7 8 9 10 POST my_index/_search { \"query\": { \"wildcard\": { \"title.keyword\": { \"value\": \"*乌兰*\" } } } } prefix（前缀匹配检索）\n用于检索以特定字符或字符串作为名称开头的文档\n1 2 3 4 5 6 7 8 9 10 POST my_index/_search { \"query\": { \"prefix\": { \"title.keyword\": { \"value\": \"乌兰\" } } } } terms set\n可以检索匹配一定数量给定词项的文档，其中匹配的数量可以是固定值，也可以是基于另一个字段的动态值\n1 2 3 4 5 6 7 8 9 10 11 POST my_index/_search { \"query\": { \"terms_set\": { \"tags\": { \"terms\": [\"喜剧\", \"动作\", \"科幻\"], \"minimum_should_match_field\": \"tags_count\" } } } } fuzzy（支持编辑距离的模糊查询）\n1 2 3 4 5 6 7 8 9 10 POST my_index/_search { \"query\": { \"fuzzy\": { \"title\": { \"value\": \"language\" } } } } IDs\n基于给定的ID组快速召回相关数据\n1 2 3 4 5 6 7 8 POST my_index/_search { \"query\": { \"ids\": { \"values\": [\"1\", \"2\", \"3\"] } } } regexp（正则匹配检索）\n1 2 3 4 5 6 7 8 9 10 POST my_index/_search { \"query\": { \"regexp\": { \"product_name.keyword\": { \"value\": \"Lap..\" } } } } 多表关联检索\nNested（Nested嵌套类型检索）\nHas child（子文档查询父文档）\nHas parent（父文档查询子文档）\nParent ID\n组合检索\nbool（组合检索）\nmust：查询结果必须满足指定条件\nmust_not：查询结果必须不满足指定条件\nfilter：过滤条件\nshould：查询结果可以满足的部分条件，具体满足条件的最小数量由minimum_should_match参数控制\nboosting（提升评分）\nconstant score\ndisjunction max\nfunction_score（自定义评分）\n相对不常用检索\n经纬度检索\n形状类型检索\n跨度检索\n特定检索\nscript（脚本检索）\nscript_score（脚本评分检索）\nmore like this（相似度检索）\npercolate\nrank feature\nwrapper\npinned query\ndistance feature\nquery和filter的区别\nquery用于评估文档相关性，并对结果进行评分，通常用于搜索场景\nfilter用于筛选文档，不会对文档评分，通常用于过滤场景\n高亮、排序和分页\n高亮语法\nfragment_size：每个高亮片段的字符数\nnumber_of_fragments：高亮最大片段数，如果片段数设置为0，则不返回任何高亮片段，而是将整个字段内容突出显示并返回，同时fragment_size将被忽略。默认值为5\nfields：待高亮字段\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 POST kibana_sample_data_ecommerce/_search { \"_source\": [\"products.product_name\"], \"query\": { \"match\": { \"products.product_name\": \"dress\" } }, \"highlight\": { \"number_of_fragments\": 0, \"fragment_size\": 150, \"fields\": { \"products.product_name\": { \"pre_tags\": [\"\u003cem\u003e\"], \"post_tags\": [\"\u003c/em\u003e\"] } } } } 排序语法\n_score：按分数排序\n_doc：按索引顺序排序（通常用在scroll遍历上）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 POST my_index/_search { \"query\": { \"match\": { \"title\": \"乌兰\" } }, \"sort\": [ { \"popular_degree\": { \"order\": \"desc\" } }, { \"_score\": { \"order\": \"asc\" } } ] } 分页语法\nfrom：表示结果集的起始位置，从0开始，默认值为0\nsize：表示每页返回的文档数量，默认值为10\n1 2 3 4 5 6 7 8 POST my_index/_search { \"from\": 0, \"size\": 10, \"query\": { \"match_all\": {} } } 自定义评分\nIndex Boost：在索引层面修改相关度\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 POST my_index_*/_search { \"indices_boost\": [ { my_index_1: 1.5 }, { my_index_2: 1.2 } ], \"query\": { \"term\": { \"subject.keyword\": { \"value\": \"subject\" } } } } boosting：修改文档相关度\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 POST my_index/_search { \"query\": { \"bool\": { \"must\": [ { \"match\": { \"title\": { \"query\": \"新闻\", \"boost\": 3 } } } ] } } } negative_boost：降低相关度\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 POST my_index/_search { \"query\": { \"boosting\": { \"positive\": { \"match\": { \"title\": \"乌兰\" } }, \"negative\": { \"match\": { \"title\": \"新闻\" } }, \"negative_boost\": 0.1 } } } function_score：自定义评分\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 POST my_index/_search { \"query\": { \"function_score\": { \"query\": { \"match_all\": {} }, \"script_score\": { \"script\": { \"source\": \"_score * (doc['sales'].value + doc['vistors'].value)\" } } } } } rescore_query：查询后二次打分\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 POST my_index/_search { \"query\": { \"match\": { \"title\": \"乌兰\" } }, \"rescore\": { \"window_size\": 50, \"query\": { \"rescore_query\": { \"function_socre\": { \"script_score\": { \"script\": { \"source\": \"doc['popular_degree'].value\" } } } } } } } 检索模板\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # 定义检索模板 POST _scripts/cur_search_template { \"script\": { \"lang\": \"mustache\", \"source\": { \"query\": { \"match\": { \"{{cur_field}}\": \"{{cur_value}}\" } }, \"size\": \"{{cur_size}}\" } } } # 基于检索模板进行检索 POST my_index/_search/template { \"id\": \"cur_search_template\", \"params\": { \"cur_field\": \"item_id\", \"cur_value\": 1, \"cur_size\": 50 } } 分页查询\nfrom+size查询\n优缺点\n优点\n支持随机翻页 缺点\n限于max_result_window设置，不能无限制翻页\n存在深度翻页问题，越往后翻页越慢\n适用场景\n小型数据集或者从大数据集中返回Top N(N≤10000)结果集的业务场景\n主流PC搜索引擎中支持随机跳转分页的业务场景\n不适用场景\n过度分页或一次请求太多结果\n原因：搜索请求通常会跨多个分片，每个分片必须将其请求的命中内容以及先前页面的命中内容加载到内存中，会显著增加内存和CPU使用率，导致性能下降 search_after查询\n执行步骤\n创建PIT视图\n1 POST kibana_sample_data_logs/_pit?keep_alive=5m 创建基础查询语句\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 GET /_search { \"size\": 10, \"query\": { \"match\": { \"host\": \"elastic\" } }, \"pit\": { # 上一步请求返回的id \"id\": \"xxx\" }, \"sort\": { \"response.keyword\": \"asc\" } } 实现后续翻页\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 GET /_search { \"size\": 10, \"query\": { \"match\": { \"host\": \"elastic\" } }, \"pit\": { \"id\": \"xxx\" }, \"sort\": { \"response.keyword\": \"asc\" }, # 上一步返回的sort结果 \"search_after\": [ \"200\", 4 ] } 优缺点\n优点\n不严格受制于max_result_window（单次请求值不能超过max_result_window），可以无限地往后翻页 缺点\n只支持向后翻页，不支持随机翻页 适用场景\n适合在手机端应用的场景中使用 scroll查询\n执行步骤\n指定检索语句的同时设置scroll上下文保留时间\n1 2 3 4 5 6 7 8 9 POST kibana_sample_data_logs/_search?scroll=3m { \"size\": 100, \"query\": { \"match\": { \"host\": \"elastic\" } } } 向后翻页，继续获取数据，直到没有要返回的结果为止\n1 2 3 4 5 6 POST _search/scroll { \"sroll\": \"3m\", # 上一步请求返回的id \"scroll_id\": \"xxx\" } 优缺点\n优点\n支持全量遍历，是检索大量文档的重要方法，但单次遍历的size值不能超过max_result_window的大小 缺点\n响应是非实时的；保留上下文需要具有足够的堆内存空间；需要通过更多的网络请求才能获取所有结果 适用场景\n大量文档检索：当要检索的文档数量很大，甚至需要全量召回数据时，scroll查询是一个很好的选择\n大量文档的数据处理：滚动API适合对大量文档进行数据处理，例如索引迁移或将数据导入其他技术栈\nElasticsearch聚合 聚合分类\n分桶聚合：用于将数据分组\nTerms分桶聚合（分组聚合结果）\nRange范围聚合（分区间聚合）\nHistogram直方图聚合（间隔聚合）\nDate histogram日期聚合（时间间隔聚合）\nDate range日期范围聚合（自定义日期范围聚合）\nComposite组合聚合（支持聚合后分页）\nFilters过滤聚合（满足给定过滤条件的聚合）\n1 2 3 4 5 6 7 8 9 10 11 POST my_index/_search { \"size\": 0, \"aggs\": { \"color_terms_agg\": { \"terms\": { \"field\": \"color\" } } } } 指标聚合：用于计算数据的指标\nAvg平均值聚合（求平均值）\nSum汇总聚合（求汇总之和）\nMax最大值聚合（求最大值）\nMin最小值聚合（求最小值）\nStats统计聚合（求统计结果值）\nTop hits详情聚合（求各外层桶的详情）\nCardinality去重聚合（去重）\nValue count计数聚合（计数）\n1 2 3 4 5 6 7 8 9 10 11 POST my_index/_search { \"size\": 0, \"aggs\": { \"max_agg\": { \"max\": { \"field\": \"size\" } } } } 管道子聚合：对其他聚合的结果进行再次计算和分析，用于对数据进行复杂的分析\nBucket selector选择子聚合\nBucket sort排序子聚合\nMax bucket最大值子聚合\nMin bucket最小值子聚合\nStats bucket统计子聚合\nSum bucket求和子聚合\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 POST my_index/_search { \"size\": 0, \"aggs\": { \"hole_terms_agg\": { \"terms\": { \"field\": \"has_hole\" }, \"aggs\": { \"max_value_agg\": { \"max\": { \"field\": \"size\" } } } }, \"max_hole_color_agg\": { \"max_bucket\": { \"buckets_path\": \"hole_terms_agg\u003emax_value_agg\" } } } } ","description":"","tags":["ElasticSearch","ES"],"title":"书籍学习-一本书讲透Elasticsearch：原理、进阶和工程实践（二）","uri":"/posts/2024-08-19-learning_es_1_2/"},{"categories":["学习笔记"],"content":" 本文是个人学习书籍《一本书讲透Elasticsearch：原理、进阶和工程实践》过程中所记录的一些笔记，内容来源于书籍\nElastic Stack全景 ELK Stack：Elasticsearch、Logstash、Kibana\nElastic Stack：Elasticsearch（存储、查询）、Logstash + Beats（采集、清洗）、Kibana（可视化）\nElasticsearch特点\nRESTful API\n横向扩展节点\n面向文档\n无模式，无须定义好字段类型、长度等\n近实时搜索\n响应快\n易扩展\n多租户\n多语言\nLogstash：提供免费且开放的服务器端数据处理管道(pipeline)，能够从多个不同的数据源采集数据、转换数据，然后将数据发送到诸如Elasticsearch等“存储库”中\nKibana：集成了丰富的可视化工具、界面交互开发工具和管理工具，可以辅助技术人员进行开发、调试和运维工作，并可以自定义各种维度的数据报表\nBeats：集合了多种单一用途的数据采集器，这些数据采集器包含轻量型日志采集器Filebeat、轻量型指标采集器Metricbeat、轻量型网络数据采集器Packetbeat等\nElastic Stack应用场景\n全文检索\n日志分析\n商业智能\nElasticsearch基础知识 搜索引擎的目标\n全面性\n速度\n准确性\n搜索引擎的核心要求\n识别用户真正的需求\n匹配用户需求\n找到可信数据\n检索质量的评价指标\n召回率：在一次返回的搜索结果中与搜索关键词相关的文档占所有相关文档的比例\n精准率：本次搜索结果中相关文档所占的比例\n倒排索引：在一个文档集合中，每个文档都可视为一个词语的集合，倒排索引则是将词语映射到包含这个词语的文档的数据结构\n全文检索：全文检索的前提是待检索的数据已经索引化，当用户查询时能根据建立的倒排索引进行查找\nElasticsearch的核心概念\n集群：一组Elasticsearch节点的集合\n节点：一个Elasticsearch实例，更确切地说，它是一个Elasticsearch进程\n索引：用于存储和管理相关数据的逻辑容器\n分片：包含索引数据的一个子集，并且其本身具有完整的功能和独立性，可以将分片近似看作“独立索引“\n副本：为了保证集群的容错性和高可用性、提高查询的吞吐率，Elasticsearch提供了复制数据的特性。分片可以被复制，被复制的分片称为“主分片“，主分片的复制版本称为“副本“\n文档：存储在Elasticsearch索引中的JSON对象\n字段：Elasticsearch中最小的单个数据单元，类似于关系型数据库表中的字段\n映射：映射类似于关系型数据库中的Schema，可以近似地理解为“表结构”\n分词：构建倒排索引的重要一环\nElasticsearch集群部署 Elasticsearch集群堆内存设置建议：将堆大小配置为服务器可用内存的50%，上限为32GB，且预留足够的内存给操作系统以提升缓存效率\n节点角色（node.roles配置项）\n主节点：关键作用主要在全局管理上，如管理索引的创建和删除、监控集群节点、确认分片分配、存储重要的元数据（索引的元数据、集群的元数据）\n专用候选主节点（master）\n仅投票主节点：仅用于投票，不会被选为主节点（master, voting_only）\n数据节点：保存数据、执行数据处理操作（data）\n内容数据节点：存储、搜索、索引数据（data_content）\n热数据节点：保存最近、最常访问的热数据（data_hot）\n温数据节点：保存访问频次低且很少更新的时序数据（data_warm）\n冷数据节点：保存不经常访问且通常不更新的时序数据（data_cold）\n冷冻数据节点：保存很少访问且从不更新的时序数据（data_frozen）\n注意：data_hot、data_warm和data_cold需要和data_content一起配置，数据的实际存储需要靠data_content角色\ningest节点：通常执行由预处理管道组成的预处理任务（ingest）\n仅协调节点：负责路由分发请求、聚合结果（配置项为空）\n远程节点：用于跨集群检索或跨集群复制（remote_cluster_client）\n机器学习节点（ml, remote_cluster_client）\n转换节点（transform）\nElasticsearch集群核心配置\nnetwork.host，不修改时默认为开发模式，节点配置错误时只会在日志中写入警告信息，节点依然能启动；修改后会升级为生产模式，一旦配置错误，则节点无法正常启动\nLinux前置配置\n修改文件描述符数量限制（调高）\n修改最大映射数量（调高）\nelasticsearch.yml配置文件\njvm.option配置文件\nElasticsearch索引 定义\n名称\nsettings\n静态设置\n动态设置\nmappings\n可以将索引映射理解成MySQL中的表结构Schema aliases\n一个索引可以创建多个别名\n一个别名也可以指向多个索引\n操作\n新增/创建索引\n方式一：详细定义索引设置、映射、别名\n方式二：只定义索引名，而settings、mappings取默认值\n删除索引\n方式一：删除索引（推荐）\n方式二：结合delete_by_query和match_all实现清空，还能保留索引\n修改索引\n查询索引\n索引别名\n索引模板\n定义\n普通模板（_index_template）\n组件模板（_component_template）\n映射下的动态模板（dynamic_templates）\nElasticsearch映射 映射的定义\n元字段\n标识元字段\n_index：表示文档所属的索引\n_id：表示文档的id\n文档源字段\n_source：表示代表文档正文的原始JSON对象\n_size：表示source字段的大小（以字节为单位）\n索引元字段\n_field_names：表示给定文档中包含非空值的所有字段\n_ignored：表示由于设置ignore_malformed而在索引时被忽略的字段\n路由元字段\n_routing：用于将给定文档路由到指定的分片 其他元字段\n_meta：表示应用程序特定的元数据\n_tier：指定文档所属索引的数据层级别\n数据类型\n基本类型\nbinary：编码为Base64字符串的二进制类型\nboolean：仅支持true和false的布尔类型\nkeyword：支持精准匹配的keyword类型、const_keyword类型和wildcard类型\nnumber：数值类型，如integer、long、float、double等\ndate：日期类型，包括date和date_nanos\nalias：别名类型，字段级别的别名\ntext：字段级别的别名\n复杂数据类型\n数组类型：Array\nJSON对象类型：Object\n嵌套数据类型：Nested\n父子关联类型：Join\nFlattened类型：将原来一个复杂的Object或者Nested嵌套多字段类型统一映射为扁平的单字段类型\n专用数据类型\n坐标数据类型：用于保存地理位置详细信息\nIP类型：表示IPV4或IPV6地址\ncompletion类型：是Elasticsearch中的一种专用字段类型，旨在实现高效的自动补全功能\n多字段类型：为相同字段生成多种数据类型\n映射类型\n动态映射：自动检测字段类型后添加新字段（boolean类型、float类型、long类型、Object类型、Array类型、date类型、字符串类型支持动态检测，除此之外的类型是不支持动态检测匹配的，会适配为text类型）\n弊端\n字段匹配不准确\n占据多余的存储空间\n映射可能错误泛滥\n静态映射：在数据建模前，需要明确文档中各个字段的类型\n忽略动态添加字段（mappings内指定dynamic为false）\n严格禁止动态添加字段（mappings内指定dynamic为“strict”）\n映射创建后还可以更新吗\n已经定义的字段在大多数情况下不能更新，除非通过reindex操作来更新映射，但有3种情况例外：\nObject对象可以添加新的属性\n在已经存在的字段里面可以添加fields，以构成一个字段多种类型\nignore_above是可以更新的\nNested类型及应用\n没有特殊的字段类型说明，那么默认写入的嵌套数据映射为Object类型，其嵌套的字段部分被扁平化为一个简单的字段名称和值列表\nNested类型是Object数据类型的升级版本，它允许对象以彼此独立的方式进行索引\nJoin类型及应用\nJoin类型的重要特点\n对于每个索引，仅允许定义一个与Join类型关联的映射\n父文档和子文档必须在同一个分片上写入索引\n一个文档可以有多个子文档，但一个子文档只能有一个父文档\n可以为已经存在的Join类型添加新的关系\n当一个文档已经成为父文档后，就可以为该文档添加子文档\nFlattened类型及应用\nFlattened字段就是用来解决字段膨胀问题的\n使用Flattened类型，Elasticsearch未对字段进行分词等处理，因此它只会返回匹配字母大小写且完全一致的结果\nFlattened类型的不足\nFlattened不支持的查询类型\n无法执行涉及数字计算的查询\n无法支持高亮查询\n尽管支持诸如term聚合之类的聚合，但不支持处理诸如histograms或date_histograms之类的数值数据的聚合\n多表关联设计\nElasticsearch多表关联方案\nNested嵌套类型\n适用于一对少量、子文档偶尔更新、查询频繁的场景\n优点\n可以将父子关系的两部分数据关联起来\n可以基于Nested类型做任何查询\n缺点\n查询相对较慢\n更新子文档时需要更新整篇文档\nJoin父子文档类型\n适用于子文档数据量明显多于父文档的数据量的场景\n优点\n父子文档可独立更新 缺点\n维护Join关系需要占据部分内存，查询较Nested类型更耗资源 宽表冗余存储\n对每个文档保持一定数量的冗余数据以避免访问时进行多表关联\n优点\n速度快，本质是以空间换时间 缺点\n索引更新或删除数据时，应用程序不得不处理宽表的冗余数据\n某些搜索和聚合操作的结果可能不准确\n业务端关联\n在应用接口层面处理关联关系，适用于数据量少的多表关联业务场景 内部数据结构\n倒排索引\n从单词到文档的映射关系的最佳实现形式\n特点\n在索引时创建\n序列化到磁盘\n全文搜索速度非常快\n不适合做排序\n默认开启\n适用场景\n文本搜索引擎\n文档检索系统\n企业内部搜索\n社交媒体分析\n新闻和论文检索\n数据挖掘\n正排索引\n在Elasticsearch中，正排索引(doc_values)就是一种列式存储结构，默认情况下每个字段的doc_values都是激活的（除了text类型）\n特点\n在索引时创建\n序列化到磁盘\n适合排序、聚合操作\n将单个字段的所有值一起存储在单个数据列中\n默认情况下，除text之外的所有字段类型均启用正排索引\n适用场景\n对一个字段进行排序、聚合\n某些过滤场景，比如地理位置过滤\n某些与字段相关的脚本计算\nfielddata\n当text字段被用于聚合、排序或脚本操作时，fielddata会按需构建相应的数据结构。通过从磁盘读取每个字段的完整倒排索引，反转词项与文档之间的关系，并将结果存储在JVM堆的内存中构建的\n特点\n仅适用于text字段类型\n在查询时创建\n是基于内存的数据结构\n没有序列化到磁盘\n默认情况下被禁用\n适用场景\n全文统计词频\n全文生成词云\n聚合、排序、脚本计算\n_source字段\n_source字段包含索引时传递的原始JSON文档主体 store字段\n默认情况下，对字段值进行索引以使其可搜索（如倒排索引），但不存储它们。字段值是_source字段的一部分，默认情况下已存储。但对于某些特殊场景，比如你只想检索单个字段或几个字段的值，而不是整个_source的值，这时store字段就派上用场了 null_value\n使用null_value参数可以用指定的值替换显式的空值，以便对其进行索引和搜索\ntext类型不支持null_value，如果需要，可以使用multi_fields，借助keyword和text组合类型达到业务需求\nElasticsearch分词 分词发生的阶段\n写入阶段\n执行检索阶段\n分词器的组成\n字符过滤（character filter，0个或多个）\n文本切分为分词（tokenizer，1个）\n分词后再过滤（token filter，0个或多个）\n分词器的分类\n支持不同语言的分词器\n默认分词器（standard分词器）：将词汇单元转换成小写，并去除停用词和标点符号\n其他典型的分词器（如IK分词等）\n分词选型注意事项\n若数据量非常少且不要求子串高亮，则可以考虑keyword\n若数据量大且要求子串高亮，则推荐使用Ngram分词，结合match或者match_phrase检索实现\n若数据量大，则不建议使用wildcard前缀匹配\nElasticsearch预处理 Elasticsearch数据预处理是指在从数据源写入Elasticsearch的中间环节对数据进行的处理操作\n预处理步骤\n定义预处理管道，通过管道实现数据预处理（_ingest/pipeline/xxx）\n写入数据关联预处理管道\n写入数据\nenrich预处理\n能在不同索引间通过相同的关联字段，从一个索引向另一个索引扩充字段信息\n组成部分\nenrich policy（_enrich/policy/xxx）\nsource index（源索引）：用于丰富新写入文档的索引\nenrich index（丰富索引）：执行enrich policy生成的索引\nElasticsearch内部管理的系统级索引\n用途很单一，仅用于enrich processor\n以.enrich-*开头\n只读，不支持人为修改\n","description":"","tags":["ElasticSearch","ES"],"title":"书籍学习-一本书讲透Elasticsearch：原理、进阶和工程实践（一）","uri":"/posts/2024-08-19-learning_es_1_1/"},{"categories":["推荐工程系列"],"content":"背景 随着业务的发展，需要使用到模型的场景越来越多，实现一个统一的模型管理平台来规范模型处理流程，提供模型从训练、发布到预测的生命周期管理，可以极大地方便算法同学的快速迭代模型。因此，从功能角度看，要求模型管理平台至少需要具备以下能力：\n提供模型生命周期管理，包含模型注册、发布、加载、预测、卸载\n保障离线训练与在线预测的一致性\n平台架构 Model Admin：模型管理后台\nModel Management：提供模型的注册、发布、模拟预测等功能\nTransformer Management：提供预处理实现在离线机器学习平台（LAI）热加载的功能\nFile Management：提供预处理文件配置管理功能\nModel Service：模型服务\nModel API：提供模型预测接口\nFeature：获取特征流程\nPreprocessing：预处理流程\nPrediction：预测流程\nModel Loading：提供模型加载/卸载\nXGBoost：XGBoost模型加载\nTensorFlow：TensorFlow模型加载\nPMML：PMML模型格式加载\nHDFS：存放由机器学习平台（LAI）训练生成的模型相关文件\nMySQL：存放模型管理后台写入模型配置等数据\nRedis：存放用户、用户-物品交叉特征\nRocksDB：存放物品特征\n核心功能 模型注册 为了方便感知线上都有哪些模型，在模型发布前，需要先在平台预先注册模型名，后续机器学习平台（LAI）的模型发布组件需要填写相关的模型名和模型配置key\n模型发布 模型发布分成两步，首先得先通过机器学习平台（LAI）的模型发布组件进行模型发布（此时的发布只是将模型相关文件发往HDFS） 然后再通过模型管理界面，手动点击发布按钮，往指定的环境发布模型（操作成功后模型服务会感知到并进行模型加载） 模型预测 模型被模型服务加载后，可以通过模型管理界面的模拟请求按钮进入模拟请求界面（判断模型是否加载完毕可以通过界面上显示的日期版本（代表的是最新版本），如下图红框，未有版本加载时显示为0），填写请求相关参数即可请求模型预测出结果\n未来规划 下沉模型预处理到C++层，优化模型预测性能\n模型请求监控\n模型部署容器化隔离\n","description":"","tags":["推荐工程","工作总结","模型管理平台"],"title":"推荐工程-模型管理平台","uri":"/posts/model_platform/"},{"categories":["数据对接系列"],"content":" 对跨系统业务数据对接流程进行标准化处理，采用配置化的方式进行数据对接需求的开发，从而提高数据对接的开发效率以及问题排查效率，减少在数据对接需求上所投入的人力成本\n背景 随着公司业务的发展，所服务的客户越来越多时，经常会遇到需要将客户业务系统中（比如ERP等系统）的数据对接到公司所提供的系统内、或者将公司内部系统的数据推送给客户的业务系统这样的需求，这部分需求往往是提供一些基础的数据，是在系统交付上线前所必须要完成的。所以每服务一个新的客户时，一般都避免不了在这一部分投入一些开发资源。因此如果能够提高这部分需求的开发效率的话，对于系统能否尽早交付有很大的帮助。\n早期公司内对于数据对接的需求处理方式最初是来一个对接需求时就定制化开发一个，后面发现对于同样的基础数据对接，其基本的对接流程都是差不多的，比如将客户业务系统的数据对接到公司内部系统的流程一般是这样的：\n从客户业务系统中读取数据\n将读取到的数据写入到数据库中间表中\n从中间表获取数据，并做一些转换或者其他的业务处理\n将处理完的数据写入到公司内部系统中\n基于上面的流程，数据对接的处理做了一些优化，引入了流程处理引擎liteflow，将流程里的第2步和第4步做通用化处理，每次新需求开发时仅需要处理第1步和第3步的业务，减少了部分的工作量。但是这个过程中还是需要做比较多代码开发，代码质量依赖于开发个人能力，并且这个对接过程相对黑箱，一旦流程出现问题时，排查时如果缺乏了一些日志记录，很难定位到问题。\n为解决以上问题，推动了数据对接平台的建设，期望平台能够满足以下核心诉求：\n标准化数据对接场景，沉淀公司内部平台对外的OpenApi接口\n针对大部分基础数据对接的需求，实现零代码开发（除标准OpenApi接口开发外），通过配置完成对接\n将对接流程透明化，即能够看到流程每一步的执行过程，出现问题时能够快速定位到问题\n平台架构 核心功能 站点 站点类似于租户的概念，在实际应用中一般会为一个企业建立一个站点，站点间的数据是可以相互隔离的，具体是可以通过数据库配置为每个站点配置独立的存储数据的数据库。\n客户端 客户端是指部署在客户环境上的程序（具体是一个可执行jar包），由它建立与客户环境上相关数据源的联系，通过采集（读数据）或出仓（写数据）任务进行相关数据的同步。客户端在部署时需要在管理平台上进行登记，获取一个身份标识（AK访问秘钥），以此来与云端服务进行交互。\n数据源 数据源是对数据的输入和输出方式的抽象，具体可以是通过Web API、数据库、消息队列等方式，每种数据源都有其独特的配置，比如Web API需要指定具体的访问域名、认证方式等，数据库需要指定数据库类型（目前支持MySQL、Oracle）、连接地址、账号密码等，消息队列需要指定消息队列类型（目前支持Kafka、Solace）、连接地址、账号密码等。\n接口 接口隶属于具体的数据源，是定义数据具体是从哪里输入或输出的，以及输入或输出的细节。以Web API数据源为例，一个真实的HTTP获取数据接口可以定义为一个读接口，需要指定具体的请求方式、请求头、请求体、返回结果字段映射等，接口定义完后是在后续的任务中使用。\n数据字典 数据字典用来管理流到云端的数据，可以是一个数据库物理表或一个常量表。通常客户端采集程序所读取并上报的数据会写到对应的数据字典表中，以供后续进行计算或出仓使用。\n采集任务 采集任务负责定义如何从外部系统采集数据到云端，这里会使用到前面所定义好的接口（具体是读接口）来采集数据，以及定义好的数据字典来存储数据，并且可以选择一个合适的采集频率执行，以此可以不断地采集新产生的数据。\n计算任务 计算任务用于数据的转换计算，如果从客户端所采集上来的数据不满足最终需要落地的数据，可以定义计算任务来处理成最终想要的数据，目前计算任务支持定义SQL来做转换，并且是可以独立执行的，不依赖于其他任务。\n出仓任务 出仓负责定义如何将云端的数据输出给外部系统，这里会从定义好的数据字典中读取数据，并使用定义好的接口（具体是写接口）来输出数据。出仓任务是以监控数据字典中数据状态的形式来执行的，当有未出仓的数据时会触发任务执行。\n任务调度记录 任务调度记录用于记录任务（采集任务、计算任务、出仓任务）每次调度的情况，包含请求内容，响应内容等关键执行日志，便于查看或排查问题。\n开放接口 开放接口是⽀持外部系统主动往平台推送数据的形式，与采集任务主动采集外部系统数据的形式相反，但最终的数据都是需要落到数据字典中存储。\n开放接口调度记录 同任务调度记录作用，记录开放接⼝每次被调度的情况，包含请求内容，响应内容等关键执行日志，便于查看或排查问题。\n用户 作为一个独立的平台，有自身的用户体系，方便后续针对用户做权限控制。\n告警 告警主要是用于当任务执⾏失败时，推送相关的告警信息到⻜书、钉钉、企微等协作平台上，以便相关责任人能够及时介入处理。\n","description":"","tags":["工作总结","数据对接"],"title":"数据对接平台","uri":"/posts/2024_05_29-data_dock_platform/"},{"categories":["采坑系列"],"content":" 采坑系列是记录日常学习/工作中所遇到的问题，可能是一个Bug、一次性能优化、一次思考等，目的是记录自己所处理过的问题，以及解决问题这一过程中所做的思考或总结，避免后续再犯相似的错误。\n问题描述 公司后端开发规范中需要有逻辑删除字段来实现软删除（即针对删除操作不会删除实际的记录，只会更新对应逻辑删除字段的值，并且在查询时自动加上过滤条件过滤掉被标识为删除的记录），项目中所使用的ORM框架为Mybatis-plus，其支持实现逻辑删除的功能，详情见官方文档。但在实际使用过程中发现一个奇怪的现象：前辈们告知逻辑删除字段需要作为实体类的最后一个属性（即放在最后），否则其功能会失效，具体原因不是很清晰。\n因个人在开发中将逻辑删除字段放在了基类中，不确定会不会影响逻辑删除功能的正常使用，本着严谨以及好奇的态度，决定实际探究一下：为什么逻辑删除字段需要作为实体类的最后一个属性？（保持质疑-。-）\n探究原因 复现 首先清楚项目中所使用的Mybatis-plus版本是3.0-RELEASE（这个很关键，因为就是它的问题！）\n然后就是通过Demo（详细见示例代码中的DemoEntityMapperTest#testSelect）复现出查询时逻辑删除功能生效与失效的现象（尝试将BaseEntity中的逻辑删除字段deleted放在最后和非最后，查看效果）\n当deleted放在最后时，从生成的SQL语句看是有自动添加逻辑删除条件的，如下图： 当deleted放在非最后时（例如移到modifyTime上面），可以看到并没有自动加上逻辑删除条件，如下图： 调试 确定入口 以上用于复现问题使用的方法为BaseMapper#selectList，根据Mybatis-plus的BaseMapper实现原理，自定义逻辑的实现会对应一个AbstractMethod实现类，又因为我们Demo中使用了LogicSqlInjector，其提供了一系列的实现类，而selectList方法对应的实现类为LogicSelectList，所以从该类进入调试\n跟踪执行 跟踪LogicSelectList类的执行可看到逻辑删除过滤条件的处理是放在父类AbstractLogicMethod的sqlWhereEntityWrapper方法中，其直接获取TableInfo类的logicDelete属性，问题中所返回的结果为false，跳过了逻辑删除过滤条件的处理。到这里可以知道为什么逻辑删除功能没有生效了，但是具体原因还得看看为什么logicDelete属性是设置为false的\n寻找根因 通过寻找对TableInfo类的logicDelete属性进行设置的地方（通过IDEA Find Usages），最终可以找到是在TableFieldInfo类中的构造函数中进行设置的，其中initLogicDelete方法就是判断字段是否有标识@TableLogic注解\n而TableFieldInfo实例创建的地方是在TableInfoHelper类的initTableFields方法中。该方法会通过反射获取实体类中的所有字段（包含父类的），然后遍历这些字段创建成对应的TableFieldInfo元数据对象，即每个字段会调用一次TableFieldInfo的构造函数，而在构造函数中会对TableInfo类的logicDelete属性直接进行覆盖赋值（参考上图）。从这里就可以看到，如果最后一个字段不是逻辑删除字段的话，TableInfo类的logicDelete属性就为false了\n到这里就知道为什么逻辑删除字段需要作为实体类的最后一个属性了，根本原因就是Mybatis-plus 3.0-RELEASE内部代码的问题，正常的话应该只需要判断有一个字段标识了逻辑删除后，后面就不应该再对logicDelete属性进行赋值了。另外从获取实体类字段列表的方法中可以看到，返回的列表会先添加实体类自身的字段，再添加父类中非重名的字段（以此类推）。因此如果实体类有父类的话，逻辑删除字段必须放在最顶级的父类中（Object之下），并且作为最后一个属性。\n结论 通过以上的分析，可以回答最初提到的问题：\nQ：为什么逻辑删除字段需要作为实体类的最后一个属性？\nA：是因为项目所使用的Mybatis-plus 3.0-RELEASE版本中有Bug所导致，其需要保证通过反射所获取的实体类字段列表中，逻辑删除字段（标识了@TableLogic注解的）是最后一个元素。\nQ：将逻辑删除字段放在了基类中，会不会影响逻辑删除功能的正常使用？\nA：如果实体类定义了基类，逻辑删除字段必须放在基类中，同样需作为最后一个属性，逻辑删除功能才能正常使用\n回答了最初的问题后，需要另外注意的是Mybatis-plus中是通过反射来获取实体类字段列表的（即Class#getDeclaredFields方法），我们实际上依赖的是这个方法返回的字段顺序，需要看该方法能否保证字段返回的顺序和我们在类中所定义的顺序一致。从该方法的注释上我们可以看到以下注释：\nThe elements in the returned array are not sorted and are not in any particular order.\n返回数组中的元素没有排序，也没有任何特定的顺序。 也就是说该方法并不会保证字段返回的顺序和我们在类中所定义的顺序一致，原因可见*Java反射中的getDeclaredFields()方法的疑问？*（不保证并不代表就会不一致，至少目前项目在用的环境能够一致-。-），因此在Mybatis-plus 3.0-RELEASE版本中即使我们保证将逻辑删除字段作为实体类的最后一个属性定义，还是存在逻辑删除功能失效的风险。如果要彻底避免这个风险，那只能升级Mybatis-plus的版本（Mybatis-plus 3.0.1版本开始已修复上述提到的Bug），但这个要结合实际情况评估升级的成本与必要性，不过对于新的项目来说，建议还是使用新版本。\n参考 MyBatis-Plus的BaseMapper实现原理\nJava反射中的getDeclaredFields()方法的疑问？\n","description":"","tags":["Mybatis-Plus","逻辑删除"],"title":"采坑系列-Mybatis-plus 3.0-RELEASE逻辑删除Bug","uri":"/posts/2023_08_30-mybatisplus_logic_delete_bug/"},{"categories":["学习笔记"],"content":" 本文是个人学习极客时间专栏《MySQL实战45讲》过程中所记录的一些笔记，内容来源于专栏\nMySQL高可用 主备延迟 在备库上执行 show slave status 命令，它的返回结果里面会显示 seconds_behind_master，用于表示当前备库延迟了多少秒。主备延迟的来源可能\n备库所在机器的性能要比主库所在的机器性能差\n备库的压力大\n大事务，如一次性地用 delete 删除太多数据，大表 DDL 等\n主备切换策略 可靠性优先策略（有不可用时间）\n判断备库 B 现在的 seconds_behind_master，如果小于某个值（比如 5 秒）继续下一步，否则持续重试这一步；\n把主库 A 改成只读状态，即把 readonly 设置为 true；\n判断备库 B 的 seconds_behind_master 的值，直到这个值变成 0 为止；\n把备库 B 改成可读写状态，也就是把 readonly 设置为 false；\n把业务请求切到备库 B。\n可用性优先策略（有可能出现数据不一致）\n不等主备数据同步，直接把连接切到备库 B，并且让备库 B 可以读写 MySQL Join Index Nested-Loop Join **可以用上被驱动表的索引。**先遍历驱动表（全表扫描），然后从驱动表中取出每行数据中的关联值，去被驱动表中查找满足条件的记录（树搜索） Block Nested-Loop Join 被驱动表上无索引可用。把驱动表的数据读入线程内存 join_buffer 中（全表扫描，存不下则分段取），把被驱动表的每一行取出来（全表扫描），跟 join_buffer 中的数据做对比，满足 join 条件的，作为结果集的一部分返回 Join 该怎么用 能不能使用 join 语句？\n如果可以使用 Index Nested-Loop Join 算法，也就是说可以用上被驱动表上的索引，其实是没问题的\n如果使用 Block Nested-Loop Join 算法，扫描行数就会过多。尤其是在大表上的 join 操作，这样可能要扫描被驱动表很多次，会占用大量的系统资源。所以这种 join 尽量不要用\n如果要使用 join，应该选择大表做驱动表还是选择小表做驱动表？\n总是应该使用小表做驱动表（在决定哪个表做驱动表的时候，应该是两个表按照各自的条件过滤，过滤完成之后，计算参与 join 的各个字段的总数据量，数据量小的那个表，就是“小表”，应该作为驱动表） 常见配置参数 join_buffer_size：控制Join Buffer的大小，调大后可以避免多次的被驱动表扫描，从而提高性能。默认值为257KB 自增主键为什么不是连续的 自增值保存在哪 MyISAM 引擎的自增值保存在数据文件中\nInnoDB 引擎的自增值，其实是保存在了内存里，并且到了 MySQL 8.0 版本后，才有了“自增值持久化”的能力\n在 MySQL 5.7 及之前的版本，自增值保存在内存里，并没有持久化。每次重启后，第一次打开表的时候，都会去找自增值的最大值 max(id)，然后将 max(id)+1 作为这个表当前的自增值\n在 MySQL 8.0 版本，将自增值的变更记录在了 redo log 中，重启的时候依靠 redo log 恢复重启之前的值\n自增值修改机制 假设，某次要插入的值是 X，当前的自增值是 Y\n如果 X \u003c Y，那么这个表的自增值不变；\n如果 X ≥ Y，那么需要把当前自增值修改为新的自增值\n新的自增值生成算法是：从 auto_increment_offset 开始，以 auto_increment_increment 为步长，持续叠加，直到找到第一个大于 X 的值，作为新的自增值。其中，auto_increment_offset 和 auto_increment_increment 是两个系统参数，分别用来表示自增的初始值和步长，默认值都是 1\n自增值修改时机 表的自增值修改是在真正执行插入数据的操作之前，且自增值不会回退（MySQL 这么设计是为了提升性能，回退需要多一次判断id存不存在或需要扩大自增id锁的范围），所以如果插入数据操作失败（如唯一键冲突）或事务回滚，那么自增主键将出现不连续的情况（但是递增的） 自增主键不连续的原因 数据插入时发生唯一索引冲突\n数据插入事务回滚\n自增主键的批量申请。对于批量插入语句（如 insert … select、replace … select、load data 语句），MySQL 有一个批量申请自增 id 的策略，第一次申请分配1个自增id，后续每次申请的 id 都是上一次的两倍，当 id 用不到时会浪费掉\n","description":"","tags":["MySQL","极客时间"],"title":"专栏学习-MySQL实战45讲（三）","uri":"/posts/2022-11-01-learning_mysql_45_3/"},{"categories":["学习笔记"],"content":" 本文是个人学习极客时间专栏《MySQL实战45讲》过程中所记录的一些笔记，内容来源于专栏\nMySQL锁 全局锁 全局锁就是对整个数据库实例加锁。MySQL提供了一个加全局读锁的方法，命令是 Flush tables with read lock (FTWRL)。该命令的典型使用场景是做全库逻辑备份。也就是把整库每个表都 select 出来存成文本。\n官方自带的逻辑备份工具是 mysqldump。当 mysqldump 使用参数 —**single-transaction **的时候，导数据之前就会启动一个事务，来确保拿到一致性视图（可重复读隔离级别下）。而由于 MVCC 的支持，这个过程中数据是可以正常更新的。但该方法只适用于所有的表使用事务引擎的库\n表级锁 MySQL表级锁有两种，一种是表锁，一种是元数据锁（Meta data lock，MDL）\n表锁的语法是 lock tables … read/write，可以用 unlock tables 主动释放锁或等待客户端断开时自动释放\n元数据锁（MDL）在 MySQL 5.5 版本引入，不需要显示使用，在访问一个表时会被自动加上。当对一个表做增删改查操作（DML）的时候，加 MDL 读锁；当要对表做结构变更操作（DDL）的时候，加 MDL 写锁。事务中的 MDL 锁，在语句执行开始时申请，但是语句结束后并不会马上释放，而会等到整个事务提交后再释放\n行锁 在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。如果事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放\n死锁应对策略\n直接进入等待，直到超时。这个超时时间可以通过参数 innodb_lock_wait_timeout 来设置\n发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑。\n加锁规则 加锁的基本单位是 next-key lock，next-key lock 是前开后闭区间\n查找过程中访问到的对象才会加锁\n索引上的等值查询，给唯一索引加锁的时候，next-key lock 退化为行锁\n索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock 退化为间隙锁\n唯一索引上的范围查询会访问到不满足条件的第一个值为止\n常见配置参数 innodb_lock_wait_timeout：设置锁申请的最长等待时间，默认值为50s\ninnodb_deadlock_detect：设置是否开启死锁检测机制，默认值为on\nMySQL视图 View 它是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果。创建视图的语法是 create view … ，而它的查询方法与表一样\n一致性读视图 用于支持 RC（Read Committed，读提交）和 RR（Repeatable Read，可重复读）隔离级别的实现。它没有物理结构，作用是事务执行期间用来定义“我能看到什么数据”\n使用 begin/start transaction 启动事务时，一致性视图是在执行第一个快照读语句时创建的\n使用 start transaction with consistent snapshot 启动事务时（在可重复读隔离级别下才有意义），一致性视图是在执行该命令时创建的\n“快照”在 MVCC 里是怎么工作的？ InnoDB 里面每个事务有一个唯一的事务 ID，叫作 transaction id。它是在事务开始的时候向 InnoDB 的事务系统申请的，是按申请顺序严格递增的。\n每行数据也都是有多个版本的。每次事务更新数据的时候，都会生成一个新的数据版本，并且把 transaction id 赋值给这个数据版本的事务 ID，记为 row trx_id。通过当前版本和 undo log可以计算出历史版本\n在实现上， InnoDB 为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活跃”的所有事务 ID。“活跃”指的就是，启动了但还没提交。数组里面事务 ID 的最小值记为低水位，当前系统里面已经创建过的事务 ID 的最大值加 1 记为高水位。这个视图数组和高水位，就组成了当前事务的一致性视图（read-view）。而数据版本的可见性规则，就是基于数据的 row trx_id 和这个一致性视图的对比结果得到的。\n对于当前事务的启动瞬间来说，一个数据版本的 row trx_id，有以下几种可能\n如果落在已提交事务区域，表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是可见的；\n如果落在未开始事务区域，表示这个版本是由将来启动的事务生成的，是肯定不可见的；\n如果落在未提交事务区域，那就包括两种情况\n若 row trx_id 在数组中，表示这个版本是由还没提交的事务生成的，不可见；\n若 row trx_id 不在数组中，表示这个版本是已经提交了的事务生成的，可见。\n通俗点的判断规则：一个数据版本，对于一个事务视图来说，除了自己的更新总是可见以外，有三种情况：\n版本未提交，不可见；\n版本已提交，但是是在视图创建后提交的，不可见；\n版本已提交，而且是在视图创建前提交的，可见。\n事务启动时根据某个条件读取到的数据，直到事务结束时，再次执行相同条件，还是读到同一份数据，不会发生变化，称为“一致性读”\n如果事务隔离级别为 REPEATABLE READ，则同一事务中的所有一致读将取该事务中第一个读请求所建立的快照\n如果事务隔离级别为 READ COMMITTED 隔离级别，则在每次读请求时都会重新创建一份快照\n更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”，除了 update 语句外，select 语句如果加锁，也是当前读\n为什么我的MySQL会“抖”一下 Innodb脏页 可能引发脏页Flush的情况\nredo log 写满了。这时候系统会停止所有更新操作，把 checkpoint 往前推进，对应的所有脏页都 flush 到磁盘上，redo log 留出空间可以继续写\n系统内存不足。当需要新的内存页，而内存不够用的时候，就要淘汰一些数据页，空出内存给别的数据页使用。如果淘汰的是“脏页”，就要先将脏页写到磁盘\nMySQL 认为系统“空闲”的时候，会进行刷脏页的操作\nMySQL 正常关闭时，会把内存的脏页都 flush 到磁盘上\n刷脏页的控制策略\nInnoDB 的刷盘速度要参考这两个因素：一个是脏页比例，一个是 redo log 写盘速度 常见配置参数 innodb_io_capacity：定义了 InnoDB 后台任务每秒可用的I/O操作数(IOPS)，比如刷新 buffer pool 的页面和合并 change buffer 中的数据。默认值为200\ninnodb_max_dirty_pages_pct：用来控制 buffer pool 中脏页的百分比，当脏页数量占比超过这个参数设置的值时，InnoDB 会启动刷脏页的操作，默认值为90\ninnodb_flush_neighbors：用来控制 buffer pool 刷脏页时是否把脏页邻近的其他脏页一起刷到磁盘，5.7版本默认值为1， 8.0版本默认值为0\n0：表示刷脏页时不刷其附近的脏页\n1：表示刷脏页时连带其附近毗连的脏页一起刷掉\n2：表示刷脏页时连带其附近区域的脏页一起刷掉，1与2的区别是2刷的区域更大一些\n为什么表数据删掉一半，表文件大小不变 删除表数据 delete 命令只是把记录的位置，或者数据页标记为了“可复用”，但磁盘文件的大小是不会变的（标记删除）。这些可以复用，而没有被使用的空间，看起来就像是“空洞”\n不止是删除数据会造成“空洞”，插入或更新数据因页分裂操作也会产生“空洞”\n重建表 可以使用 alter table t engine=InnoDB 命令来重建表\n重建表的流程（Online DDL）\n建立一个临时文件，扫描表 A 主键的所有数据页\n用数据页中表 A 的记录生成 B+ 树，存储到临时文件中\n生成临时文件的过程中，将所有对 A 的操作记录在一个日志文件（row log）中\n临时文件生成后，将日志文件中的操作应用到临时文件，得到一个逻辑数据上与表 A 相同的数据文件\n用临时文件替换表 A 的数据文件\n常见配置参数 innodb_file_per_table：用来控制表数据是存在共享表空间里，还是单独的文件，默认值为ON\nON：每个 InnoDB 表数据存储在一个以 .ibd 为后缀的文件中\nOFF：表的数据放在系统共享表空间，也就是跟数据字典放在一起\ncount计数 count(*)的实现方式 MyISAM 引擎把一个表的总行数存在了磁盘上，因此执行 count(*) 的时候会直接返回这个数，效率很高（没有where条件的情况）\nInnoDB 引擎执行 count(*) 的时，需要把数据一行一行地从引擎里面读出来，然后累积计数（因为MVCC，无法像 MyISAM 那样）\n不同的 count 用法 count() 的语义：count() 是一个聚合函数，对于返回的结果集，一行行地判断，如果 count 函数的参数不是 NULL，累计值就加 1，否则不加。最后返回累计值\ncount(主键 id)：InnoDB 引擎会遍历整张表，把每一行的 id 值都取出来，返回给 server 层。server 层拿到 id 后，判断是不可能为空的，就按行累加\ncount(1)：InnoDB 引擎遍历整张表，但不取值。server 层对于返回的每一行，放一个数字“1”进去，判断是不可能为空的，按行累加\ncount(字段)：InnoDB 引擎会遍历整张表，把每一行的字段值都取出来，返回给 server 层。server 层拿到字段值后，判断是不可能为空的，就按行累加。\ncount(*)：并不会把全部字段取出来，而是专门做了优化，不取值。count(*) 肯定不是 null，按行累加\n按照效率排序的话，count(字段) \u003c count(主键id) \u003c count(1) ~= count(*)\n“order by”是怎么工作的 全字段排序 VS rowid 排序 全字段排序：server 从引擎获取查询的所有字段（含排序字段）放入 sort_buffer 中，再根据排序字段进行排序，按照排序结果返回给客户端\nrowid 排序：server 从引擎获取排序字段和主键id放入 sort_buffer 中，再根据排序字段进行排序，按照排序结果拿 id 的值回到原表中取出待查询的字段返回给客户端（对于InnoDB 表来说，rowid 排序会要求回表多造成磁盘读，因此不会被优先选择）\n常见参数配置 sort_buffer_size：MySQL 为排序开辟的内存（sort_buffer）的大小。如果要排序的数据量小于 sort_buffer_size，排序就在内存中完成。但如果排序数据量太大，内存放不下，则不得不利用磁盘临时文件辅助排序。默认值为256KB 为什么只查一行的语句，也执行这么慢 查询长时间不返回 等MDL锁（show processlist 命令查看 Waiting for table metadata lock 的状态，处理方式就是找到谁持有 MDL 写锁，然后把它 kill 掉）\n**等 flush **（show processlist 命令查看 Waiting for table flush 的状态（出现 Waiting for table flush 状态的可能情况是：有一个 flush tables 命令被别的语句堵住了，然后它又堵住了我们的 select 语句，处理方式就是干掉阻塞源头）\n等行锁\n查询慢 无索引，走全表扫描\n当一致性读需要回溯的版本很多时，会比较慢\n短期提高性能的做法 短连接风暴 处理掉那些占着连接但是不工作的线程\n减少连接过程的消耗，如让数据库跳过权限验证阶段\n慢查询性能问题 索引没有设计好。通过紧急创建索引来解决\nSQL 语句没写好。通过改写 SQL 语句来处理，MySQL 5.7 提供了 query_rewrite 功能，可以把输入的一种语句改写成另外一种模式\nMySQL 选错了索引。使用查询重写功能，给原来的语句加上 force index\nQPS 突增问题 数据库端下掉业务的处理方法\n白名单机制。如果能够确定业务方会下掉这个功能，只是时间上没那么快，那么就可以从数据库端直接把白名单去掉\n账号机制。如果这个新功能使用的是单独的数据库用户，可以用管理员账号把这个用户删掉，然后断开现有连接\n把压力大的SQL使用查询重写做降级\nI/O性能问题 设置 binlog_group_commit_sync_delay 和 binlog_group_commit_sync_no_delay_count 参数，减少 binlog 的写盘次数。可能会增加语句的响应时间，但没有丢失数据的风险。\n将 sync_binlog 设置为大于 1 的值（比较常见是 100~1000）。这样做的风险是，主机掉电时会丢 binlog 日志\n将 innodb_flush_log_at_trx_commit 设置为 2。这样做的风险是，主机掉电的时候会丢数据\n","description":"","tags":["MySQL","极客时间"],"title":"专栏学习-MySQL实战45讲（二）","uri":"/posts/2022-11-01-learning_mysql_45/"},{"categories":["学习笔记"],"content":" 本文是个人学习书籍《深入理解Java虚拟机》过程中所记录的一些笔记，内容来源于书籍\n虚拟机字节码执行引擎 运行时栈帧结构 局部变量表：用于存放方法参数和方法内部定义的局部变量，以Slot为最小单位\n操作数栈：用于存放方法执行过程中所做的各种运算的操作数以及结果\n动态连接：每个栈帧都包含一个指向运行时常量池中该栈帧所属方法的引用，持有这个引用是为了支持方法调用过程中的动态连接\n方法返回地址\n附加信息\n方法调用 解析（静态）：调用目标在程序代码写好、编译器进行编译时就确定下来的\n静态方法（invokestatic）\n私有方法、实例构造器、父类方法（invokespecial）\nfinal修饰的方法（invokevirtual）\n分派（静态+动态）\n静态分派：依赖静态类型来定位方法执行版本的分派，典型应用是方法重载\n动态分派：运行期根据实际类型确定方法执行版本的分派，典型应用是方法重写\n程序编译与代码优化 早期（编译器）优化 Javac编译器 编译过程\n解析与填充符号表过程\n插入式注解处理器的注解处理过程\n分析与字节码生成过程\n晚期（运行期）优化 即时编译器 解释器与编译器\n分层编译\n第0层，程序解释执行\n第1层，也称C1编译，将字节码编译为本地代码，进行简单、可靠的优化\n第2层（或2层以上），也称C2编译，也是将字节码编译为本地代码，但会启用一些编译耗时较长的优化，甚至会根据性能监控信息进行一些不可靠的激进优化\n热点探测\n基于采样的热点探测\n基于计数器的热点探测（HotSpot采用）\n方法计数器：用于统计方法被调用的次数（默认会有热度衰减，即超过一定时间限度不满足阈值条件时次数将会被减半，可见统计的不是方法的绝对调用次数）\n回边计数器：用于统计一个方法中循环体代码执行的次数（默认没有热度衰减，因此统计的是该方法循环执行的绝对次数）\n编译优化技术\n公用子表达式消除\n如果表达式E已经计算过了，且到现在E中所有变量的值没有发生变化，那么E的这次出现就成为了公共子表达式 数组范围检查消除\n方法内联\n逃逸分析\n为优化代码手段提供依据的分析技术，基本行为就是分析对象动态作用域\n如果能证明一个对象不会逃逸到方法或线程之外，可为这个变量进行一些高效的优化\n栈上分配\n同步消除\n标量替换\n高效并发 Java内存模型与线程 Java内存模型 目的\n屏蔽掉各种硬件和操作系统的内存访问差异，以实现让Java程序在各种平台下都能达到一致的内存访问效果 主内存和工作内存\n所有共享变量都存储在主内存，每条线程都有自己的工作内存\n工作内存中保存了该线程使用到的变量的主内存副本拷贝\n线程对变量的所有操作都必须在工作内存中进行，而不能直接读写主内存中的变量\n内存间交互操作\nlock：作用于主内存的变量，把一个变量标识为线程独占状态\nunlock：作用于主内存的变量，把一个处于锁定状态的变量释放出来\nread：作用于主内存的变量，把一个变量的值从主内存传输到线程的工作内存中，以便随后的load操作使用\nload：作用于工作内存的变量，把read操作从主内存中得到的变量值放入工作内存的变量副本中\nuse：作用于工作内存的变量，把工作内存中一个变量的值传递给执行引擎\nassign：作用于工作内存的变量，把一个从执行引擎接收到的值赋给工作内存的变量\nstore：作用于工作内存的变量，把工作内存中一个变量的值传输到主内存中，以便随后的write操作使用\nwrite：作用于主内存的变量，把store操作从工作内存中得到的变量的值放入主内存的变量中\nvolatile特性/语义\n保证变量对所有线程的可见性，即当线程修改了变量的值后，新值对于其他线程来说是立即得知的\n禁止指令重排序优化\n先行发生原则（用来确定一个访问在并发环境下是否安全）\n程序次序规则\n在一个线程内，按照程序代码顺序，书写在前面的操作先行发生于书写在后面的操作 管程锁定的规则\n一个unlock操作先行发生于后面对同一个锁的lock操作 volatile变量规则\n对一个volatile变量的写操作先行发生于后面对这个变量的读操作 线程启动规则\nThread对象的start方法先行发生于此线程的每个动作 线程终止规则\n线程中的所有操作都先行发生于对此线程的终止检测，我们可以通过Thread.join()方法结束、Thread.isAlive()的返回值等手段检测到线程已经终止执行 线程中断规则\n对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生，可以通过Thread.interrupted()方法检测到是否有中断发生 对象终结规则\n一个对象的初始化完成先行发生于它的finalize()方法的开始 传递性\nJava与线程 Java线程调度 抢占式线程调度 线程的实现 使用内核线程的实现\n使用用户线程的实现\n使用用户线程加轻量级进程混合实现\nJava线程的实现：基于操作系统原生线程模型来实现\n状态转换 线程状态\n新建（New）：创建后未启动的线程处于这种状态\n运行（Runable）：Runable包括了操作系统线程状态中的Running和Ready\n无限等待（Waiting）：处于这种状态的线程不会被分配CPU执行时间，它们要等待被其他线程显示地唤醒\n没有设置Timeout参数的Object.wait()方法\n没有设置Timeout参数的Thread.join()方法\nLockSupport.park()方法\n限期等待（Timed Waiting）：处于这种状态的线程不会被分配CPU执行时间，不过无须等待被其他线程显示地唤醒，在一定时间之后它们会由系统自动唤醒\nThread.sleep()方法\n设置了Timeout参数的Object.wait()方法\n设置了Timeout参数的Thread.join()方法\nLockSupport.parkNanos()方法\nLockSupport.parkUntil()方法\n阻塞（Blocked）：线程被阻塞了\n终止（Terminated）：已终止线程的状态\n线程安全与锁优化 Java中的线程安全 不可变\n绝对线程安全\n相对线程安全\n保证对这个对象单独的操作式线程安全的，不需要做额外的保障措施，但对于一些特定顺序的连续调用，可能需要在调用端使用额外的同步手段来保证 线程兼容\n对象本身并不是线程安全的，但可以通过在调用端正确地使用同步手段来保证对象在并发环境中可以安全地使用 线程对立\n无论调用端是否采取了同步措施，都无法在多线程环境中并发使用的代码 线程安全的实现方法 互斥同步\nsynchronized\nReentrantLock\n非阻塞同步\nCAS 无同步\n可重入代码：如果一个方法，它的返回结果是可以预测的，只要输入了相同的数据，就都能返回相同的结果\n线程本地存储\nThreadLocal 锁优化 自旋锁与自适应自旋 自旋锁：线程在获取不到锁时不进行阻塞，而是执行一个忙循环（自旋），等待锁的释放\n自适应自旋：自旋时间不固定，而是由前一次在同一个锁上的自旋时间及锁的拥有者状态来决定\n锁消除 锁消除：虚拟机即时编译器在运行时，对一些代码上要求同步，但是被检测到不可能存在共享数据竞争的锁进行消除，主要判断依据来源于逃逸分析的数据支持 锁粗化 锁粗化：如果一系列的连续操作都对同一对象反复加锁和解锁，虚拟机会扩展锁的范围，以减少频繁地进行互斥同步操作导致不必要的性能损耗 轻量级锁 目的：在没有多线程竞争的前提下，减少传统的重量级锁使用操作系统互斥量产生的性能消耗\n加解锁过程\n加锁\n在当前线程栈帧中创建锁记录（Lock Record）的空间，用于存储锁对象目前Mark Word的拷贝\n使用CAS尝试将对象的Mark Word更新为指向Lock Record的指针，操作成功即线程获得该对象的锁，操作失败再检测Mark Word是否已指向当前线程的栈帧，如果是则说明当前线程已经拥有了这个对象的锁，否则说明锁被其他线程抢占了，要膨胀为重量级锁\n解锁\n使用CAS操作把对象当前的Mark Word和线程中复制的Displaced Mark Word替换回来，如果替换成功，整个同步过程就完成了。如果替换失败，说明有其他线程尝试过获取该锁，那就要在释放锁的同时，唤醒被挂起的线程 偏向锁 目的：消除数据在无竞争情况下的同步原语，进一步提高程序的运行性能\n偏向过程\n当锁对象第一次被线程获取时，虚拟机会把对象头标志设为偏向模式，同时使用CAS操作把线程ID记录到对象的Mark Word中，如果操作成功，则持有偏向锁的线程以后进入这个锁相关的同步块时，可以不进行任何同步操作\n当有另外一个线程尝试获取这个锁时，偏向模式就宣告结束\n","description":"","tags":["JVM","Java虚拟机"],"title":"书籍学习-深入理解Java虚拟机（二）","uri":"/posts/2022-10-19-learning_jvm_2/"},{"categories":["学习笔记"],"content":" 本文是个人学习书籍《深入理解Java虚拟机》过程中所记录的一些笔记，内容来源于书籍\n自动内存管理机制 Java内存区域与内存溢出异常 运行时数据区域 线程共享\n方法区\n类信息、常量、静态变量、即时编译后的代码\nJava7前实现是永久代（PermGen），Java8后实现是元空间（Metaspace）\nOutOfMemoryError\n无法满足内存分配需求 堆\n对象实例\nOutOfMemoryError\n没有内存完成实力分配且无法再扩展 直接内存（不是虚拟机规范中定义的内存区域）\n线程私有\n虚拟机栈\n栈帧\n局部变量表\n64位long和double类型的数据会占用2个局部变量空间（Slot） 操作数栈\n动态链接\n方法出口\nStackOverflowError\n线程请求的栈深度大于虚拟机所允许的深度 OutOfMemoryError\n扩展时无法申请到足够的内存 本地方法栈\nHotSpot将其与虚拟机栈合在一起 程序计数器\n如果线程正在执行的是Java方法，记录的是正在执行的虚拟机字节码指令的地址\n如果线程正在执行的是Native方法，记录值为未定义（Undefined）\nHotSpot虚拟机对象探秘 对象的创建\n检查常量池是否有类的符号引用、检查引用代表的类是否已被加载、解析和初始化过\n以上检查不通过时，执行类加载过程\n为新生对象分配内存\n指针碰撞：用过的内存放在一边，空闲的内存放在另一边\n空闲列表：维护一个列表，记录哪些内存块是可用的\n初始化零值\n设置对象头\n执行方法\n对象的内存布局\n对象头\n对象运行时数据（Mark Word）\n哈希码\nGC分代年龄\n锁状态标志\n线程持有的锁\n偏向线程ID\n偏向时间戳\n…\n类型（类元数据）指针\n实例数据\n对齐填充\n对象的访问定位\n句柄：reference存储的是到对象的句柄地址\n到对象实例数据的指针\n到对象类型数据的指针\n直接指针：reference存储的是到对象的直接地址（HotSpot使用这种）\n实战OutOfMemoryError异常\nJava堆溢出（-Xms、-Xmx）\n-XX:+HeapDumpOnOutOfMemoryError：出现内存溢出异常时Dump出当前的内存堆转储快照\n通过内存映像分析工具对快照进行分析，确认是出现内存泄漏还是内存溢出\n如果是内存泄漏，可进一步查看泄漏对象到GC Roots的引用链\n如果是内存溢出，检查堆参数是否可以调大或尝试减少程序运行时的内存消耗\n虚拟机栈和本地方法栈溢出（-Xss）\n栈深度超过虚拟机所允许的最大深度，抛出StackOverflowError\n建立过多线程导致OutOfMemoryError，在不能减少线程数的情况下，可通过减少最大堆和减少栈容量来换取更多的线程\n方法区和运行时常量池溢出（-XX:PermSize、-XX:MaxPermSize、-XX:MetaspaceSize、-XX:MaxMetaspaceSize）\n大量字符串添加到常量池（Java7前）\n动态生成大量的Class\n本地直接内存溢出（-XX:MaxDirectMemorySize）\nDirectByteBuffer\nunsafe.allocateMemory\nHeap Dump文件中不会看见明显的异常，且Dump文件较小\n垃圾收集器与内存分配策略 对象存活 引用计数算法\n给对象添加一个引用计数器，当有地方引用它时，计数器值加1，当引用失效时，计数器值减1\n很难解决对象间循环引用问题\n可达性分析算法\n通过一系列“GC Roots”对象为起始点向下搜索，所走过的路径称为引用链，当一个对象到”GC Roots”没有任何引用链相连时，则此对象是不可用的\n可作为GC Roots的对象\n虚拟机栈（栈帧中的本地变量表）中引用的对象\n方法区中类静态属性引用的对象\n方法区中常量引用的对象\n本地方法栈中JNI引用的对象\n引用\n强引用：不会被垃圾收集器回收\n软引用：当将要发生内存溢出时，垃圾收集器会二次回收这些对象\n弱引用：当垃圾收集器工作时，都会回收这些对象\n虚引用：无法通过引用获取对象实例，唯一的目的是能在这个对象被垃圾收集器回收时收到一个系统通知\n生存还是死亡\n宣告一个对象死亡至少需要经过两次标记过程，当没有GC Roots引用链时会进行第一次标记，执行finalize方法（对象覆盖finalize方法且虚拟机未调用过）后会进行第二次标记，如果对象被重新关联到GC Roots上，则不用回收 回收方法区\n回收废弃常量和无用的类\n无用的类\n类所有实例都已被回收\n加载该类的ClassLoader已被回收\n类对应的Class对象没有在任何地方被引用\n垃圾收集算法 标记-清除算法\n首先标记出所有需要回收的对象，标记完成后进行统一回收\n不足点\n效率问题\n空间问题：内存碎片\n复制算法（为解决标记-清除的效率问题）\n将可用内存分成大小相等的两块，每次只使用其中的一块，当这一块内存用完后就将还存活的对象复制到另一块上，然后再把已使用过的内存空间一次清理掉 标记-整理算法\n与标记-清除类似，不过不是直接对可回收对象进行清理，而是让所有存活对象都向一端移动，然后直接清理掉端边界外的内存 分代收集算法\n垃圾收集器 Serial + Serial Old\n单线程，新生代采用复制算法，老年代采用标记-整理算法，用于Client模式（桌面应用）较多 ParNew\n简单理解为多线程版本的Serial收集器，用于新生代，常与CMS搭配使用 Parallel Scavenge + Parallel Old\n关注点为达到一个可控的吞吐量，而不是停顿时间 CMS(Concurrent Mark Sweep)\n基于标记-清除算法实现，可分为4个步骤\n初始标记（STW）\n标记GC Roots能直接关联到的对象 并发标记\n进行GC Roots Tracing的过程 重新标记（STW）\n修正并发标记期间因用户线程导致标记变动的那一部分对象的标记记录 并发清除\n缺点\n对CPU资源敏感\n无法处理浮动垃圾（垃圾出现在标记过程之后，无法在当次收集中处理），可能出现”Concurrent Mode Failure”失败而导致另一次Full GC的产生（当CMS运行期间预留的内存无法满足程序需要时，就会出现一次”Concurrent Mode Failure”失败，这时虚拟机将临时启用Serial Old来重新进行老年代的垃圾收集）\n-XX:CMSInitiatingOccupancyFraction设置老年代使用了多少（百分比）空间后触发CMS收集 空间碎片\n-XX:+UseCMSCompactAtFullCollection设置CMS在进行Full GC时开启内存碎片的合并整理，默认是开启的\n-XX:CMSFullGCsBeforeCompaction设置执行多少次不压缩的Full GC后，跟着来一次带压缩的，默认值是0，表示每次都进行\nG1\n特点\n并行与并发\n分代收集\n空间整合\n整体看是基于标记-整理算法，局部（两个Region之间）看是基于复制算法 可预测的停顿\n在后台建立一个Region优先列表，每次根据允许的收集时间，优先回收价值最大的Region 步骤\n初始标记（STW）\n标记GC Roots能直接关联到的对象，并且修改TAMS（Next Top to Mark Start）的值，让下一阶段用户程序并发运行时，能在正确可用的Region中创建对象 并发标记\n从GC Roots开始对堆中对象进行可达性分析，找出存活的对象 最终标记（STW）\n修正在并发标记中因用户程序继续运行而导致标记产生变动的那一部分标记记录 筛选回收（STW）\n对各个Region的回收价值和成本进行排序，根据用户期望的停顿时间制定回收计划 内存分配与回收策略 对象优先在Eden分配\n大多数情况下，对象在新生代Eden区中分配，当Eden区没有足够空间进行分配时，虚拟机将发起一次Minor GC 大对象直接进入老年代\n**-XX:PretenureSizeThreshold **设置内存大于这个值的对象直接在老年代分配，只对Serial和ParNew两款收集器有效 长期存活的对象进入老年代\n**-XX:MaxTenuringThreshold **设置对象年龄大于这个值的对象晋升到老年代 动态对象年龄判定\n如果在Survivor空间中相同年龄所有对象大小综合大于Survivor空间的一半，则年龄大于或等于该年龄的对象就可以直接进入老年代 空间分配担保\n老年代最大可用的连续空间 \u003e 新生代所有对象总空间\n老年代最大可用的连续空间 \u003e 历次晋升到老年代对象的平均大小\n虚拟机性能监控与故障处理工具 JDK的命令行工具 jps：虚拟机进程状况工具\n可以列出正在允许的虚拟机进程，并显示虚拟机执行主类名以及传递给主类的参数或启动时的JVM参数 jstat：虚拟机统计信息监视工具\n用于监视虚拟机各种运行状态信息，如类加载、内存、垃圾收集、JIT编译等 jinfo：Java配置信息工具\n实时地查看和调整虚拟机各项参数 jmap：Java内存映像工具\n用于生成堆转储快照，或查询finalize执行队列、Java堆和永久代的详细信息，如空间使用率、当前用的哪种收集器等 jhat：虚拟机堆转储快照分析工具\njstack：Java堆栈跟踪工具\n用于生成虚拟机当前时刻的线程快照 JDK的可视化工具 JConsole：Java监视与管理控制台\n基于JMX的可视化监控、管理工具 VisualVM：多合-故障处理工具\n虚拟机执行子系统 类文件结构 无关性的基石 虚拟机和字节码存储格式\n平台无关性\n语言无关性\nClass类文件的结构 采用类C语言结构体的伪结构来存储数据，只有两种数据类型\n无符号数：属于基本的数据类型，以u1/u2/u4/u8表示n字节的无符号数，用来描述数字、索引引用、数量值或者按照UTF-8编码的字符串值\n表：由多个无符号数或其他表作为数据项构成的复合数据类型，以”_info”结尾，用来描述有层次关系的复合结构的数据\n魔数与Class文件的版本\nClass文件头4个字节为魔数，用来确定这个文件是否为一个能被虚拟机接受的Class文件\n魔数后4个字节为Class文件的版本号（次版本号+主版本号）\n常量池\n主要存放两大类常量：字面量和符号引用 访问标志\n常量池后2个字节为访问标志，用于识别一些类或接口层次的访问信息，如Class是类还是接口，是否public、abstract、final等 类索引、父类索引与接口索引集合\n类索引用于确定这个类的全限定名，父类索引用于确定这个类的父类的全限定名，接口索引集合用来描述这个类实现/继承了哪些接口 字段表集合\n字段表用于描述接口或类中声明的变量（类变量+实例变量，不会列出从父类继承来的变量），每一项由字段修饰符+字段的简单名称+字段/方法的描述符（+属性表集合）组成 方法表集合\n虚拟机类加载机制 类加载的时机 类加载生命周期\n加载 → 验证 → 准备 → 解析 → 初始化 → 使用 → 卸载 必须立即对类进行”初始化”的情况\n遇到new、getstatic、putstatic或invokestatic这4条字节码指令时\n使用java.lang.reflect包的方法对类进行反射调用时\n当初始化一个类时发现父类还没有初始化，则需先初始化父类\n用户指定的执行主类（main方法的那个类）\n如果一个java.lang.invoke.MethodHandle实例最后的解析结果REF_getStatic、REF_putStatic、REF_invokeStatic的方法句柄对应的类没有进行初始化，则需先触发\n类加载的过程\n加载\n通过类全限定名获取二进制字节流\n将字节流所代表的静态存储结构转换为方法区的运行时数据结构\n在内存（HotSpot存放在方法区）生成一个Class对象，作为方法区这个类的各种数据的访问入口\n验证（-Xverify:none可以关闭大部分的类验证）\n确保Class文件的字节流中包含的信息符合当前虚拟机的要求\n验证动作\n文件格式验证（基于二进制字节流进行）\n验证字节流是否符合Class文件格式的规范，并且能被当前版本的虚拟机处理 元数据验证（基于方法区的存储结构进行）\n对字节码描述的信息进行语义分析，以保证符合Java语言规范的要求 字节码验证（基于方法区的存储结构进行）\n通过数据流和控制流分析，确定程序语义是合法的、符合逻辑的 符号引用验证（基于方法区的存储结构进行）\n对类自身以外（常量池中的各种符号引用）的信息进行匹配性校验 准备\n正式为类变量分配内存并设置类变量初始值（零值）的阶段，这些类变量所使用的内存都将在方法区中进行分配 解析\n将常量池内的符号引用替换为直接引用的过程 初始化\n执行类构造器方法的过程\n方法的细节\n由编译器自动收集所有类变量的赋值动作和静态语句块中的语句合并产生，静态语句块中只能访问定义在它之前的变量，不能访问但可赋值定义在它之后的变量\n虚拟机会保证在执行子类的方法前，父类的方法已经执行完毕\n如果一个类没有静态语句块也没有对类变量的赋值操作，编译器可以不为这个类生成方法\n执行接口（接口的实现类）的方法不需要先执行父接口的方法，只有当父接口中定义的变量使用时，父接口才会初始化。\n虚拟机会保证一个类的方法在同一类加载器下只会执行一次\n类加载器 类和类加载器\n对于任意一个类，都需要由加载它的类加载器和这个类本身一同确立其在Java虚拟机中的唯一性，每一类加载器，都拥有一个独立的类名称空间 双亲委派模型\n系统提供的类加载器\n启动类加载器\n加载\u003cJAVA_HOME\u003e/lib目录（或-Xbootclasspath参数指定的路径）下虚拟机所识别（通过文件名识别）的类库 扩展类加载器\n加载\u003cJAVA_HOME\u003e/lib/ext目录（或java.ext.dirs系统变量指定的路径）下的所有类库 应用程序（系统）类加载器\n加载用户类路径（Classpath）下的所有类库 双亲委派模型（非强制性的约束模型）\n除顶层的启动类加载器外，其余的类加载器都应当有自己的父类加载器，这里的父子关系一般是以组合的关系来复用父加载器的代码\n双亲委派工作流程：一个类加载器收到类加载的请求时，会把这个请求委派给父类加载器去完成，只有当父类加载器反馈无法完成加载请求时，子加载器才会尝试自己去加载\n自定义类加载器（遵循双亲委派模型）：继承ClassLoader类，重写findClass方法\n破坏双亲委派模型\n自定义类加载器（破坏双亲委派模型）：继承ClassLoader类，重写loadClass方法\nServiceLoader使用线程上下文加载器去加载SPI代码（即父类加载器请求子类加载器完成类加载）\n","description":"","tags":["JVM","Java虚拟机"],"title":"书籍学习-深入理解Java虚拟机（一）","uri":"/posts/2022-10-19-learning_jvm_1/"},{"categories":["学习笔记"],"content":" 本文是个人学习极客时间专栏《MySQL实战45讲》过程中所记录的一些笔记，内容来源于专栏\nMySQL基础架构 Server层 涵盖MySQL大多数功能，所有跨存储引擎的功能都在这一层实现，比如存储引擎、触发器、视图等\n连接器：管理连接，权限校验\n查询缓存：命中则直接返回结果（MySQL 8.0已删除这一块）\n分析器：词法分析，语法分析\n优化器：执行计划生成，索引选择\n执行器：操作引擎，返回结果\n…\n存储引擎层 负责数据的存储和提取，提供读写接口，插件式架构\nMySQL日志 redo log InnoDB引擎特有的日志；记录的是“在某个数据页上做了什么修改”；空间固定且是循环写；用于保证即使数据库发生异常重启，之前提交的记录都不会丢失（crash-safe）。\n当有一条记录需要更新时，InnoDB会把记录写到redo log（先写到log buffer，即写入到log buffer划分的众多的redo log block，解决直接写磁盘带来的性能损耗**）**里面，并更新内存，这个时候更新就算完成了。同时，InnoDB 引擎会在适当的时候，将这个操作记录更新到磁盘里面。\nredo log 的写入机制\n事务在执行过程中，生成的 redo log 是要先写到 redo log buffer （线程共享）的，之后再经过 write 操作写入到文件系统的 page cache，fsync 操作持久化到磁盘，write 和 fsync 的时机，可由参数 innodb_flush_log_at_trx_commit 控制\n让一个没有提交的事务的 redo log 写入到磁盘中的场景\nInnoDB 有一个后台线程，每隔 1 秒，就会把 redo log buffer 中的日志，调用 write 写到文件系统的 page cache，然后调用 fsync 持久化到磁盘\nredo log buffer 占用的空间即将达到 innodb_log_buffer_size 一半的时候，后台线程会主动写盘，因事务还未提交，这个写盘动作只是 write\n并行的事务提交的时候，顺带将这个事务的 redo log buffer 持久化到磁盘\n组提交机制\nbinlog MySQL Server层的实现，所有引擎都可以使用；记录的是语句的原始逻辑，比如“给 ID=x 这一行的 c 字段加 1 ”；追加写不会覆盖以前的日志；用于归档，主从数据同步。\nbinlog 的写入机制\n事务执行过程中，先把日志写到 binlog cache（线程独占），事务提交的时候，再把 binlog cache 写到 binlog 文件中，写到 binlog 文件分 write 和 fsync，其中 write 是写到文件系统的 page cache，fsync 才是把数据持久化到磁盘，write 和 fsync 的时机，是由参数 **sync_binlog **控制\n组提交机制，可调整 binlog_group_commit_sync_delay 和 binlog_group_commit_sync_no_delay_count 参数，减少 binlog 的写盘次数\nbinlog 的存储格式\nstatement：binlog 里面记录的就是 SQL 语句的原文，语句在不同实例执行的结果可能会不一致，从而可能导致数据不一致\nrow：binlog 里面记录了真实行的信息，可避免数据不一致，也可用来恢复数据，但比较占空间和IO资源（MySQL 5.7 开始默认使用的格式）\nmixed：MySQL 自己会判断这条 SQL 语句是否可能引起主备不一致，如果有可能，就用 row 格式，否则就用 statement 格式\nredo log和binlog的写入顺序 - redo log两阶段提交 Innodb引擎生成redo log，将其置为prepare状态\n执行器生成binlog并写入磁盘\n执行器调用引擎的提交事务接口，引擎把刚写入的redo log改为commit状态\n常见配置参数 innodb_flush_log_at_trx_commit：设置事务提交时redo log磁盘写入策略，默认值为1\n0：提交事务时不立即把redo log buffer里的数据写入日志文件，而是依靠主线程每1s执行一次写入并刷新到磁盘，MySQL宕机时会丢失1s数据\n1：提交事务时会把redo log buffer里的数据写入日志文件，并且会执行fsync（阻塞操作）强制将os buffer刷新到磁盘\n2：提交事务时会把redo log buffer里的数据写入日志文件，但不会执行fsync强制将os buffer刷新到磁盘，而是依靠主线程每1s执行一次刷新到磁盘，MySQL宕机时可能会丢失1s数据\nsync_binlog：设置事务提交时binlog磁盘写入策略，默认值为0\n0：当事务提交之后，MySQL不做fsync之类的磁盘同步指令刷新binlog_cache中的信息到磁盘，而让Filesystem自行决定什么时候来做同步，或者cache满了之后才同步到磁盘\n1：当每进行1次事务提交之后，MySQL将进行一次fsync之类的磁盘同步指令来将binlog_cache中的数据强制写入磁盘\nn：当每进行n次事务提交之后，MySQL将进行一次fsync之类的磁盘同步指令来将binlog_cache中的数据强制写入磁盘\nbinlog_cache_size：控制单个线程内 binlog cache 所占内存的大小。如果超过了这个参数规定的大小，就要暂存到磁盘。默认值为32KB\ninnodb_log_buffer_size：控制 InnoDB 写入磁盘上日志文件的缓冲区大小。默认值为16M\nbinlog_group_commit_sync_delay：控制 binlog 提交在调用 fsync 之前等待的微秒数。默认值为0\nbinlog_group_commit_sync_no_delay_count：控制累积多少次事务以后才调用 fsync ，如果binlog_group_commit_sync_delay 设置为 0 ，则该参数设置无效。默认值为0\nMySQL事务 隔离性与隔离级别 多个事务同时执行时，可能出现脏读、不可重复读、幻读的问题，为了解决这些问题，就有了“隔离级别”，SQL标准的事务隔离级别包含：\n读未提交（Read Uncommitted）：一个事务还未提交时，它做的变更可以被其他事务看到\n读已提交（Read Committed）：一个事务提交之后，它做的变更才能被其他事务看到\n可重复读（Repeatable Read）：一个事务在执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的（MySQL默认隔离级别）\n串行化（Serializable）：对于同一记录，写会加“写锁”，读会加“读锁”，当出现锁冲突时，后访问的事务必须等前一个事务执行完成才能继续进行\n默认隔离级别 Oracle默认隔离级别为Read Committed，Oracle支持Read Committed、Serializable和Read-Only，Serializable和Read-Only显然都是不适合作为默认隔离级别的，那么就只剩Read Committed这个唯一的选择）\nMySQL默认隔离级别为Repeatable Read，MySQL早期只有statement这种binlog格式，这时候如果使用读提交(Read Committed)、读未提交(Read Uncommitted)这两种隔离级别可能会出现主从数据不一致问题（出现事务乱序时）\n事务的启动方式 显示启动事务语句，begin或start transaction\nset autocommit = 0，这个命令会将这个线程的自动提交关掉，意味着如果你只执行一个select语句，这个事务就启动了，并且不会自动提交，事务将持续到你主动执行 commit 或 rollback 语句，或者断开连接\n幻读 欢读的定义：指的是一个事务在前后两次查询同一个范围的时候，后一次查询看到了前一次查询没有看到的行\n在可重复读隔离级别下，普通的查询是快照读，是不会看到别的事务插入的数据的。因此，幻读在“当前读”下才会出现\n幻读仅专指“新插入的行”\n幻读的问题\n破坏语义上行的加锁声明\n破坏数据的一致性\n幻读的解决\nInnoDB 引入了新的锁，也就是间隙锁 (Gap Lock)。跟间隙锁存在冲突关系的，是“往这个间隙中插入一个记录”这个操作，间隙锁之间都不存在冲突关系\n间隙锁和行锁合称 next-key lock，每个 next-key lock 是前开后闭区间\n间隙锁是在可重复读隔离级别下才会生效的。如果把隔离级别设置为读提交的话，就没有间隙锁了。但同时，要解决可能出现的数据和日志不一致问题，需要把 binlog 格式设置为 row\n常见配置参数 transaction-isolation：设置事务隔离级别，默认值为repeatable-read MySQL索引 索引的常见模型 哈希表：以键值存储数据的结构，做区间查询的速度很慢（因链表无序），适用于只有等值查询的场景\n有序数组：在等值查询和范围查询场景中的性能都很好，但更新数据时成本较高（需要挪动记录），只适用于静态存储引擎（即数据不会修改的场景）\n搜索树：二叉搜索树的父节点左子树所有结点的值小于父节点的值，右子树所有结点的值大于父节点的值，大多数的数据库存储并不使用二叉树，而是使用“N叉树”（通过改变 key 值或改变页大小可以调整N值）。其原因是，索引不止存在内存中，还要写到磁盘上，使用”N叉树”可以减少树高，从而减少磁盘读写次数\nInnodb的索引模型 InnoDB 使用了 B+ 树索引模型，每一个索引对应一棵 B+ 树\n根据叶子节点的内容，索引类型分为主键索引（聚簇索引）和非主键索引（二级索引）\n主键索引的叶子节点存的是整行数据，非主键索引的叶子节点内容是主键的值\n索引维护 页分裂：在无序插入新数据时，如果索引数据页已经满了，根据 B+ 树的算法，这时候需要申请一个新的数据页，然后挪动部分数据过去，这个过程称为页分裂\n页合并：当相邻两个页由于删除了数据，利用率很低之后，会将数据页做合并。合并的过程，可以认为是分裂过程的逆过程\n为什么建议建表语句里要有自增主键 从性能角度看，自增主键的插入数据模式为递增插入的场景，每次插入一条新纪录，都是追加操作，都不涉及到挪动其他记录，也不会触发叶子节点的分裂（写数据成本低/索引维护成本低）\n从存储角度看，普通索引的叶子节点需要存储主键，主键长度越小，普通索引占用的空间也就越小\n性能优化 覆盖索引：当查询字段在索引上可以直接获取结果，不需要回表操作时（即索引“覆盖”了我们的查询需求），我们称该索引为覆盖索引，覆盖索引可以减少树的搜索次数，显著提升查询性能\n最左前缀原则：只要满足索引的最左前缀，就可以利用索引来加速检索。这个最左前缀可以是联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符\n索引下推：在 MySQL 5.6 引入的索引下推优化，可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数\n普通索引和唯一索引怎么选 从业务角度考虑，如果字段需要数据库来保证唯一性，则只能选择唯一索引\n如果字段由业务代码保证唯一性，则可从两种索引的查询和更新性能角度考虑\n对于查询过程，唯一索引找到第一个满足条件的纪录后，就会停止检索；普通索引找到第一个满足条件的纪录后，需要继续往后找，直到碰到不满足条件的纪录。虽然普通索引检索次数相比唯一索引多，但因Innodb引擎是按页读写数据的，所以性能上的差异不大\n对于更新过程，当纪录要更新的目标页不在内存时，唯一索引需要将数据页读入内存，判断到没有冲突，再更新这个值；普通索引可以利用** change buffer **机制，将更新记录在 change buffer 就可返回。利用 change buffer 机制，普通索引更新性能相比唯一索引会高点\nchange buffer的使用场景：对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时 change buffer 的使用效果最好，常见的就是账单类、日志类的系统\nredo log 主要节省的是随机写磁盘的 IO 消耗（转成顺序写），而 change buffer 主要节省的则是随机读磁盘的 IO 消耗\n索引选择异常和处理 大多数时候优化器都能找到正确的索引，但偶尔会碰到索引选择错误的问题，可尝试通过以下几种方法处理\n由于索引统计信息不准确导致的问题，可以用 analyze table 来解决\n对于其他优化器误判的情况，可以采用以下方法处理\n采用 force index 强行选择一个索引\n考虑修改语句，引导 MySQL 使用我们期望的索引\n在有些场景下，可以新建一个更合适的索引，来提供给优化器做选择，或删掉误用的索引\n字符串创建索引 直接创建完整索引，这样可能比较占用空间\n创建前缀索引，节省空间，但会增加查询扫描次数，并且不能使用覆盖索引\n倒序存储，再创建前缀索引，用于绕过字符串本身前缀的区分度不够的问题\n创建 hash 字段索引，查询性能稳定，有额外的存储和计算消耗，跟第三种方式一样，都不支持范围扫描\n索引不生效的场景 条件字段函数操作：对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能，即使是对于不改变有序性的函数，也不会考虑使用索引\n隐式类型转换：where条件左侧因隐式类型转换操作使用了函数操作，同第一条规则\n在 MySQL 中，字符串和数字做比较的话，是将字符串转换成数字 隐式字符编码转换：where条件左侧因隐式字符编码转换操作使用了函数操作，同第一条规则\n常见配置参数 innodb_change_buffer_max_size：设置 Change Buffer能占用 Buffer Pool 的最大比例，默认值为25 ","description":"","tags":["MySQL","极客时间"],"title":"专栏学习-MySQL实战45讲（一）","uri":"/posts/2022-10-19-learning_mysql_45/"},{"categories":["Git"],"content":"配置步骤 1. 分别生成两份密钥文件 git bash下执行以下命令，并将生成的文件分开存放：\n1 2 ssh-keygen -t rsa -C 'your gitlab email' ssh-keygen -t rsa -C 'your github email' 2. 设置SSH Key 将生成的两份id_rsa.pub文件内容分别对应复制粘贴到github和gitlab的SSH Key配置下\n3. 将key在本地存储起来 git bash下执行以下命令，ssh-add后的路径为上面生成的id_rsa文件的路径：\n1 2 3 ssh-agent -s ssh-add ~/.ssh/github/id_rsa ssh-add ~/.ssh/id_rsa PS：如果执行ssh-add时报了以下错误，需先执行： ssh-agent bash 命令\nCould not open a connection to your authentication agent.\n4. .ssh目录下创建config文件来管理key 在用户家目录下的.ssh目录（如果不存在可自己创建）下创建config文件，内容如下（需自行替换对应的内容）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 Host github.com // 不动 HostName ssh.github.com // 不动 User xxxx@xxx.com // 你自己的github邮箱 PreferredAuthentications publickey // 不动 IdentityFile ~/.ssh/github/id_rsa // github用的rsa文件路径 Port 443 // 如果ssh -T git@github.com的时候报 ssh: connect to host github.com port 22: Operation timed out就把Port这条加上吧 Host 192.168.0.231 // 你们公司gitlab的ip地址 HostName 192.168.0.231 //与Host保持一致 User xxx@xxxx.com // 你gitlab的邮箱 IdentityFile ~/.ssh/id_rsa // gitlab用的rsa文件路径 Port 64222 // 你们公司gitlab的ip端口 5. 验证 git bash下执行以下命名验证配置：\n1 2 ssh -T git@github.com ssh -T git@gitlab的hostname 如果有看到successfully或Welcome的相关信息，说明以及配置成功\n设置仓库使用的提交用户 针对项目配置 在初始化项目（本地创建项目、通过git clone或IDEA下载仓库）后，可以通过以下命令设置该项目提交时使用的用户，git bash下执行：\n1 2 git config --local user.name 'your name' git config --local user.email 'your email' 命令执行后会修改项目根目录下的.git目录下的config文件，你也可以直接修改这个文件，加入以下配置：\n1 2 3 [user] name = your name email = your email 全局配置 如果不想针对一个一个项目设置，可以通过配置全局的参数对每个项目生效（相当于缺省的默认参数），但缺点就是只能设置一个用户，可以通过以下命令设置全局使用的用户，git bash下执行：\n1 2 git config --global user.name 'your name' git config --global user.email 'your email' Conditional Includes 在 git 2.13 版本中，增加了 conditional includes 配置，可针对不同的根目录使用不同的.gitconfig配置文件，这样就可以针对不同项目使用不同的用户了。比如针对github和gitlab项目配置不同的用户，修改用户家目录下的.gitconfig文件，加入以下配置（项目存放路径需自己替换）：\n1 2 3 4 [includeIf \"gitdir:D:/Dev/github/\"] path = .github [includeIf \"gitdir:D:/Dev/gitlab/\"] path = .gitlab 在用户家目录下分别创建.github和.gitlab，并加入以下配置：\n.github 1 2 3 [user] name = your github name email = your github email .gitlab 1 2 3 [user] name = your gitlab name email = your gitlab email 配置完成后可以到配置的目录下执行以下命令进行校验是否生效：\n1 2 git config --show-origin --get user.name git config --show-origin --get user.email 推荐资料 Git 进阶指南\nLearn Git Branching\n","description":"","tags":["Git","Gitlab","Github"],"title":"配置同时使用gitlab和github","uri":"/posts/2022-05-18-config_gitlab_and_github/"},{"categories":["推荐工程系列"],"content":" 基于 Solrcloud 研发的大规模分布式搜索引擎平台，提供简单、高效、稳定、低成本和可扩展的搜索解决方案（核心：将不同数据源（Hive、MySQL、Kafka、文件等）的数据导入到Solr集群，对外提供检索服务）\n背景 随着公司业务增长，需要对外提供站内搜索等搜索功能，早期维护人员采用了Solr作为检索引擎，搭建了Solrcloud集群，满足了当时的使用场景。但随着搜索的业务逐渐增多，在集群使用上存在的诸多问题逐渐暴露出来，比如每个业务接入都要手写程序同步数据（效率问题）、Collection数据来源分散难管理（管理问题）、查看数据需要到Solr自身的管理界面上查询（安全问题）等。为规范集群使用，解决以上众多问题，推动了搜索平台的建设，期望实现以下核心功能诉求：\n提供异构数据源数据自动同步到Solr，无需编写任何代码\n对外提供数据检索服务，屏蔽Solr自身检索的复杂性\n平台架构 架构图由下向上分别是：\n存储层 存储层包含Hive、MySQL、HBase、Kafka、HDFS等外部数据源，以及平台内部的Solr集群\n平台目前暂不支持同步HBase数据源的数据，目前HBase主要用于连接（join）获取额外字段\n索引层 索引层由多个子模块组成，分别处理来自不同数据源的数据，最终导入到Solr集群中\nHive → Solr：该模块基于开源的Hive-Solr组件进行数据同步\nMySQL → Solr：该模块基于Sqoop将MySQL数据导入到Hive临时表，再利用Hive → Solr模块导入数据，增量部分利用MySQL Binlog → Solr模块处理\nMySQL Binlog → Solr：该模块通过客户端消费统一的Canal Kafka Topic，使用Solr API写入数据\nKafka → Solr：该模块通过客户端消费指定的Kafka Topic，使用Solr API写入数据\n文件 → Solr：该模块通过读取上传文件的数据，使用Solr API写入数据\n接入层 接入层包含提供Web界面的Admin模块以及提供API接口的Service模块\nAdmin：提供业务接入、数据查询/导出、索引触发/查询、集群节点注册/监控等功能\nService：提供HTTP接口和RPC接口，包含查询、以及部分更新功能\n核心功能 业务管理 业务是对外部数据源接入平台时的一个统称，每个业务都会对应一个外部数据源、一个Solr Collection以及其他的一些配置信息（如同步信息等），用户使用平台需要先走业务接入申请，然后由管理员审核（主要是针对用户的场景审核配置是否合理）激活，激活后用户可手动进行同步或等待定时任务生效进行同步。\n业务接入需要填写的信息包含三个部分：接入信息、数据来源、同步信息\n业务接入-接入信息：\n业务名称：对业务的一个简要描述\n索引表名：Solr Collection名\n业务描述：对业务的一个详细描述\n业务类型：区分不同使用场景的业务，后续会针对不同类型的业务对Collection分配不同数量的分片\n总数据量级：对数据量级的一个大致评估，后续会针对不同量级的业务对Collection分配不同数量的分片\n分片数量：强制指定Collection的分片数量，未指定时会根据业务类型和数据量级进行决定\n告警邮件：索引失败时接收通知的邮件，这里设计得有点不好，直填邮件的方式，容易因人员离职后邮件失效影响通知\n业务接入-数据来源：\nHive接入是目前平台使用最多的接入方式，平台会从Hive对应的元数据库中读取Hive库、表、字段供用户选取\n集群：因集群迁移需要所冗余的字段，因之前只支持单集群\n数据库：对应Hive库\n数据表：对应Hive表\n字段：对应Hive字段\n主键：选择当成主键的Hive字段\n是否连接额外字段：可以关联指定Hive表获取额外字段（目前跟业务绑定，只提供了固定的表）\n只索引字段：指定字段只需要建索引，不需要存储，默认会存储\n只存储字段：指定字段只需要存储，不需要建索引，默认会索引\nMySQL接入是另外一种常见的接入方式，其填写的信息除多了MySQL连接信息外，其他的与Hive接入方式一样，平台会根据用户填入的数据库地址去读取MySQL库、表、字段供用户选取\nMySQL Binlog接入需要先将指定库表注册到平台的Canal处理程序上，由Canal程序统一处理Binlog发送到指定的Kafka上，后续再由Kafka消费模块进行处理\nKafka接入是由接入方定义好消息模板（定义需要写入的字段），平台根据消息模板解析出对应的字段，然后再从消费的消息中取出数据，通过Solr API写入数据（批量写入，会有一定延迟）\nRPC接入是由接入方通过平台对外提供的RPC接口写入数据（实时写入）\n文件接入按照平台字段规范组织数据，并将文件通过接口上传到指定位置，同步程序检测到文件后会进行处理\n业务接入-同步信息：\n同步信息跟随接入方式不同，需要填写的信息也随之不同，以下是Hive接入方式需要填写的信息：\n同步开始时间：指定定时同步开始时间\n同步间隔时间：指定定时同步的间隔时间\n同步分区：指定需要同步的分区，一般是指定最新分区，即会在同步的时候选择读取最新分区的数据\n分区字段：指定作为分区的字段\n同步方式：增量或全量，增量在更新时会覆盖主键相同的数据，全量在更新时会先删除数据再导入\n保留时间：在增量同步方式下启用，选择数据需要保留的天数\n目前保留指定天数采用的是按天生成分片，定时清理过期的分片\n索引管理 当手动或定时触发业务同步时，会生成一条索引任务，通过界面可以查看业务最新的以及历史的索引任务进行情况（耗时、执行状态等），如果想了解索引执行的详细过程也可以通过索引日志查看，平台会记录索引过程中的关键日志。如果用户不想索引任务继续执行的话，可以在界面上操作终止索引\n业务数据 当索引任务执行完成，数据成功同步到平台的Solr集群后，可通过界面查看/查询具体数据，也可导出自己想要的数据\n有时候用户会想要知道某个字段的数据分布，比如某个类别字段在线上分别有哪些类别，因此平台也提供了分组统计的功能，并通过图表形式展示数据的分布情况\n集群管理 集群管理主要包含对集群节点的注册、监控与告警，在创建Collection时会根据节点的负载情况选择合适的节点存放\n展望未来 平台建设初期主要是为了支撑内部的推荐和搜索业务，随着平台核心功能逐步稳定，也开始对其他外部系统开放使用，整体反馈效果良好，但平台在某些地方还是存在设计缺陷，要推广出去乃至作为数据中台的在线检索解决方案还需持续改进，从目前来看，平台至少可以在以下的几个方面做得更好：\n丰富支持的数据源类型，完善数据源扩展机制。当前Hive接入只支持平台内置的数据源，不支持动态扩展；缺乏其他常用数据源的支持，如PostgreSQL。\n提供灵活的告警机制，如指定通知发送模板、触发点、发送方式等。当前告警只提供了邮件方式且只有在失败的情况下才发。\n提升系统可用性。考虑索引节点宕机时如何迁移索引以保证可用等问题\n","description":"","tags":["推荐工程","工作总结","搜索平台"],"title":"推荐工程-搜索平台","uri":"/posts/2022-04-06-search_platform/"},{"categories":["推荐工程系列"],"content":" 基于Google重叠实验论文的（简化）实现，提供简单、高效的实验接入方式，支持同时进行大量线上实验，实验效果及时反馈等\n背景 早期的AB实验一般都是单层实验，逻辑实现简单，比如直接对用户流量进行分组，即可假设所有用户固定分桶区间为[0, 99]，每个实验组为区间内的一个不重叠子集，然后使用userId/deviceId计算hash值对分组数取模，得出用户所命中的实验组。这种方法易用且具有相对的灵活性，但会存在以下的问题：\n扩展性差，只能同时支持少量实验。虽然可以将流量分成很多组，但实验组一般都会要求样本数不能太少，这对本身流量较少的业务来说，分组的数量会受到限制。\n流量饥饿问题：总流量固定，如果某些实验组需要比较大的流量进行实验，则需要减少其他实验组的流量，导致其他实验组只有少量流量甚至没有流量可用。\n流量偏置问题：比如某个实验把大部分男性用户都获取了，导致其他实验很少甚至没有男性用户的样本，实验结果会有偏差。\n为解决以上问题，需要搭建一个能支持大量线上实验且能够保障独立实验间不会相互影响的平台，通过调研发现，Google发布的论文【Google 重叠实验框架：更多，更好，更快地实验】（译文地址）已经为如何实现这样的一个平台提供了相关实践，因此我们基于该论文以及自身业务的特点搭建了分层实验平台，使用该平台替代了旧的单层实验方式，大大的提升了做线上实验的效率。\n平台架构 平台整体架构比较简单，主要以下三层结构：\n存储层 该层只依赖MySQL数据库，用来存放平台所生产的相关数据。\n接口层 接口层包含对外提供服务的Http接口以及语言相关的SDK包，应用可以通过以上两种方式进行接入，此外，平台也开放了相关写入API，方便接入方通过编码的方式生成场景或实验的相关配置。\n展示层 展示层提供面向用户的界面功能，包含应用管理、场景管理、参数管理、实验记录、实验效果和白名单管理功能，基本覆盖日常实验所需功能。\n核心功能 应用管理 应用是平台用来管理不同项目（可能是一个服务或多个服务）的接入信息所抽象出来的概念，每个接入的项目需要创建对应的应用，不同应用的数据是隔离的（没有应用的权限则看不到应用数据），负责人或管理员可以将应用权限授权给其他用户。\n应用列表页：\n场景管理 场景是归属于应用内的数据，对应于实际线上需要进行实验的某个场景，比如猜你喜欢推荐场景等，这里的场景也是Google论文里面所提到的默认域的实现。\n场景列表页：\n进入到某个场景的内部，就是实际的编辑实验的位置，这里我们对于论文里面所提到的域、层、发布层以及实验都做了实现，但也根据自身业务做了一些精简，比如限制了层内不能再嵌套域、参数只能在默认域（默认参数）、实验（实验参数）和发布层（灰度参数）存在。\n树形图的编辑方式，可快速构建层级结构或修改单一元素：\n目录树的编辑方式，可同时对多个元素进行修改：\n另外，当实验越来越多时所需要的参数也随之变多，如果每次都在实验上直接修改，久而久之可能会忘记实验参数是来源于哪个实验内容，对此我们弄了针对实验参数单独管理的功能，通过独立出实验配置参数，可以追溯每个实验内容所涉及到的参数。\n实验参数列表页：\n当配置好实验参数并且测试/预发环境对实验组进行验证后，可以选择发布到线上的环境，每次发布（包含上线、下线、全量）实验会记录对应的实验内容，一个是方便产品或相关开发知道线上存在哪些实验，另一个是后续实验效果的统计会基于这些记录进行。\n实验记录列表页：\n实验效果页：\n白名单管理 白名单是用来强制指定某个用户所命中的实验组，不需要经过hash分组过程，常用来测试或验证实验组所修改的东西。\n白名单列表页：\n展望未来 目前平台已经满足主要业务的实验需求，核心功能已逐渐成熟稳定，平台所得到的反馈也更多的是在接入和使用流程方面，因此未来可以考虑在用户使用体验、平台周边工具这方面进行迭代优化，可以是以下的几个点：\n提供通用的场景配置模板，提高场景配置效率\n提升实验效果统计数据的实时性（目前一般是T+1计算）\n提供流量评估相关的工具等\n","description":"","tags":["推荐工程","工作总结","实验平台"],"title":"推荐工程-实验平台","uri":"/posts/2022-03-30-experiment_platform/"},{"categories":["推荐工程系列"],"content":" 对推荐和搜索业务处理通过DAG图可视化方式组织，提供统一对外服务接口、DAG图执行引擎，处理如并发执行、日志上报、监控等业务无关需求，让使用者可专注于业务开发\n背景 随着公司业务的发展，有越来越多的业务场景需要接入推荐，推荐场景最初的开发模式是推荐算法和工程同事根据需求开发完成推荐接口，最后与业务服务端同事进行对接，在这个开发模式下遇到了众多问题：\n针对不同推荐场景需要单独开一个接口，接口协议按需制定，每开发完⼀个接口都要重新对接⼀遍\n每个推荐场景内的处理流程是个黑盒，众多处理逻辑耦合在⼀起，重用困难，开发非常耗时\n针对较复杂的推荐场景往往会有很多处理逻辑，为了保持接口性能，需要考虑并发处理等非业务相关的功能，无法专注于业务开发\n推荐逻辑多而杂，仅以接口为粒度的监控太粗，难以定位接口内耗时较多的逻辑，优化受阻；针对部分逻辑（如某一路召回）无法在出错时做熔断处理，导致因部分出错而影响到整个接口的响应\n为解决以上问题，工程同事推动了策略平台的建设，期望满足以下核心诉求：\n推荐服务对外抽象出⼀个通用接口，与业务方只需对接一次接口协议\n每个推荐场景通过DAG图方式组织执行，由一个个逻辑处理模块组成，处理流程可视化，模块可复用\n提供统一的DAG执行引擎，处理并发、日志上报等非业务相关的功能，让开发者可专注于业务开发\n提供统一的监控、分析功能，为性能优化提供指引，支持针对单一模块、单一业务场景的熔断，以保证接口正常运行\n平台架构 核心功能 业务场景管理 业务场景是用来表示某个推荐的场景或构成推荐的部分逻辑，比如综合-首页推荐（推荐场景）、固定位-首页固定位召回（推荐的部分逻辑）等，调用方会通过业务场景来识别具体的推荐。\nPS：在平台接入多个不同项目（App）后，业务将对应于某个App，场景则对应于App下的不同推荐场景\n业务场景列表页：\n项目管理 以上的业务场景只是描述了某个推荐的场景，具体在推荐做了什么事，则是由项目来说明的。项目中核心的组成部分是模块/模块实例，每个项目由众多个模块/模块实例组成一个有向无环图，完成一个推荐场景（部分逻辑）所做的事情。\n项目列表页：\n项目编辑页：\n模块管理 模块是项目中执行业务逻辑的最小单元，每个模块对应到项目代码中的某个类，模块的key为类的名称，模块可附带一些自定义的参数，方便在运行过程动态修改。 PS：为解决同一模块可在同一项目中存在多个，引入了模块实例，模块实例可完成的功能与它依赖的模块一样，可简单理解为是多带了标识的模块。\n模块列表页：\n模块编辑页：\n推荐模拟 推荐模拟是用来模拟调用推荐接口的一个工具，通过调用推荐的rpc接口，可查看到推荐返回的结果，方便测试、检验推荐效果等。\n模拟推荐页：\n日志查询 日志查询提供了查看推荐过程及状态的功能，可查询某个用户的推荐，便于开发测试验证以及辅助定位线上问题等。\n日志查询页：\n模块数据 模块数据可实时查询每个业务场景、模块的耗时情况，也可查看两者的历史耗时情况，便于做性能优化。\n模块数据页：\n展望未来 因平台是从0到1构建的，初期为满足业务需求，平台与其他平台或业务存在较多的耦合，以至于整体功能/架构比较臃肿，且部分实现未考虑周全，现平台已逐步稳定运行，未来可考虑在以下方面进行进一步优化：\n优化整体架构，往可插拔的架构发展，平台核心只提供统一的DAG执行引擎，其他部分功能可通过插件按需扩展\n沉淀一些可复用的模块，供各业务场景使用\n提供模块热更新等动态加载功能，减少更新服务的频率\n","description":"","tags":["推荐工程","策略平台","工作总结"],"title":"推荐工程-策略平台","uri":"/posts/2022-03-26-strategy_platform/"},{"categories":["推荐工程系列"],"content":" 对线上特征统一进行管理，规范特征生产、存储、查询流程，降低特征的使用成本和维护成本\n背景 在推荐业务场景日益增多的情况下，推荐所需要的特征也在不断增多，现有的特征使用情况逐渐暴露了以下使用问题：\n写入比较分散，每个人都生产自己需要的特征，写入格式杂乱，不能达到很好的特征复用\n特征存储在HBase上，HBase集群运维成本大，偶尔发生不稳定的情况（因写入大影响到查询等），导致大量特征查询超时\n特征使用情况不明确，不能很清楚各个服务对特征的依赖\n为解决以上问题，工程同事推动了特征平台的建设，期望满足以下核心诉求：\n化零为整：提供web界面，统一管理特征的读写\n稳定可靠：降低特征服务读取延时\n平台架构 架构图由下向上分别是：\n存储层 Kafka：用于接收实时特征\nRedis、HBase：用于存放特征（离线+实时），提供在线查询，其中离线特征在Redis中采取压缩手段以节省内存\nHive：用于存放离线特征\nRocksdb：用于存放物品特征（离线+实时），提供在线查询，替换Redis中存储的物品特征\n计算层 Spark Streaming、Flink：用于生产实时特征\nSpark SQL：用于同步Hive离线特征到在线的存储\n业务处理层 实时处理程序：接收Kafka实时特征，写入Redis\n离线处理程序：同步Hive离线特征，写入Redis\n索引处理程序：接收Kafka实时特征 + 同步Hive离线特征，写入Rocksdb\n接入层 Admin：特征管理界面，提供离线/实时特征注册、特征集配置、特征查询功能\nService：提供特征查询Rpc接口\n核心功能 特征生产 离线特征 目前在离线特征生产方面工程同学暂时还未有过多的介入，这方面的特征大都是算法同学通过Hive SQL计算提取出来，工程同学暂仅限于提供一些UDF函数支持。算法同学在这一过程最终会生产出多张Hive特征表，供后续流程使用。\n实时特征 对于实时特征方面，工程同学尝试利用Spark Streaming结合Spark SQL定义一个规范处理流程（统一输入+SQL+统一输出），让算法同学可以在界面上编写SQL来生产实时特征，同时工程同学可以在流程内加入一些监控、告警等辅助功能，下面是Spark Streaming程序定义的界面：\n基本信息：填写Spark Streaming程序的一些基础信息\n补充数据信息：可以从其他数据源（如HBase）关联补充一些数据\n计算信息：编写SQL\n输出信息：指定输出结果存储到哪里\n功能上线后陆续接入了几个生产程序，整体效果还是可以的，相比之前每个算法同学都自己编写和维护自己的Spark Streaming程序，使用这种方式明显减少了算法同学的开发以及维护成本。\n特征存储 基于Redis存储 特征类型：离线+实时\n特征版本：单版本，用新数据直接覆盖旧数据，实现简单，对物理存储占用较少，但在数据异常时无法快速回滚\n特征序列化\n单表离线特征：hash结构，key为物品id，field为离线特征表id，value为所有特征列拼接的值（colConfigId\\001colValue\\001colValue，其中colConfigId为列配置记录id，\\001为特殊分隔符，colValue为列值，colValue为null时用\\002填充，采用Snappy压缩value以节省空间）\n交叉表离线特征：hash结构，key为物品id，field为交叉物品id，value为所有特征列拼接的值（colConfigId\\001colValue\\001colValue，其中colConfigId为列配置记录id，\\001为特殊分隔符，colValue为列值，colValue为null时用\\002填充，采用Snappy压缩value以节省空间）\n单表实时特征：hash结构，key为物品id_rt（_rt为后缀字符串，与离线特征区分），field为实时字段名，value为实时字段值\n交叉表实时特征：hash结构，key为物品id_rt（_rt为后缀字符串，与离线特征区分），field为交叉物品id，value为实时字段key-value json串\nPS：离线特征过期时间为7天、实时特征过期时间为3天，具体根据实际业务场景设置\n实现方式\n离线：利用SparkSQL读取Hive表数据通过Redis Pipline录入到Redis\n实时：利用Spark Streaming生成实时特征发往Kafka，消费Kafka数据录入到Redis\n优缺点\n优点\n实现简单\n性能良好\n缺点\n用特殊值拼接各列值存在风险\n不支持异常快速回滚\n量大时更新较慢（为防止影响线上读控制了写入速率）\n内存占用较多\n基于Rocksdb存储 特征类型：离线+实时\n特征版本：多版本，每一份数据对应特定版本，虽然物理存储占用较多，但在数据异常时可通过版本切换的方式快速回滚\nPS：离线按天生成版本，全实时/离线+全实时按每6小时生成一个版本，具体根据实际业务场景调整\n特征序列化\nKV存储，Value使用自定义的序列化方式以二进制的方式紧凑存储，Value的存储格式为：col_len + col_1_sign + col_1_val + col_2_sign + col_2_val + … + col_n_sign + col_n_val（根据Schema定义的字段拼接字段值，col_len为定义的字段数，col_i_sign为字段是否为空标识，col_i_val为字段值）\n支持多种数据类型的序列化\nstring：len + val_bytes\nlist：len + val_1_type + val_1 + val_2 + … + val_n\nnumber：val_bytes\nmap：len + map_val_type + map_val_1 + map_val_2 + … + map_val_n\nmap_val：str_key + val 实现方式\n离线：由BuildService读取Hive Metadata并解析Hdfs File数据生成Key-Value录入到Rocksdb Table，录入完成后将Rocksdb Table在本地对应的文件打包压缩上传到Hdfs上，最终由OnlineService下载到本地并在内存中构建Rocksdb Table提供在线服务\n实时：由BuildService消费Kafka数据录入到Rocksdb Table，消费到一定时间或数据时会生成一个版本，接着将Rocksdb Table在本地对应的文件打包压缩上传到Hdfs上，由OnlineService下载到本地并在内存中构建Rocksdb Table提供在线服务，同时OnlineService也会基于下载版本所消费到的offset继续消费数据往表中插入/更新数据\n优缺点\n优点\n内存占用较少\n性能良好\n多版本支持异常回滚\n缺点\n实现较复杂，需要进行调优 特征查询 特征查询需要先通过界面配置申请特征集（多个特征）查询key，一个key可以关联多个特征，使用方调用特征查询服务时只需传入这个key，服务内部会根据key去获取关联的特征，拿到结果后返回给使用方，下面是特征集的定义和展示界面：\n展望未来 目前整个特征平台还不是很完善的，比如说离线特征生产这一块，完全还是由算法同学自己去生产，未实现任何工程保障，可能出现类似特征重复生产、特征异常未及时发现等问题。另外，在特征迁移到本地存储后，索引构建时间，特征查询毛刺现象等这些都是需要进行优化的。总的来说，特征平台未来可以在以下方面持续改进：\n特征生产流程工程化\n减少特征存储索引构建时间\n提升特征查询服务的性能\n特征监控\n","description":"","tags":["特征平台","推荐工程","工作总结"],"title":"推荐工程-特征平台","uri":"/posts/2022-03-13-feature_platform/"},{"categories":["采坑系列"],"content":" 采坑系列是记录日常学习/工作中所遇到的问题，可能是一个Bug、一次性能优化、一次思考等，目的是记录自己所处理过的问题，以及解决问题这一过程中所做的思考或总结，避免后续再犯相似的错误。\n问题描述 近期线上的排序服务在某个时间点内出现了大量特征获取超时告警，大概是集中在中午12点左右和下午4点左右，告警持续一会后就自动恢复了，下面是某一天的告警情况：\n服务获取特征是以本地存储（基于Rocksdb）的方式获取的，且内存缓存配置得足够多，数据基本都是从内存中读取，耗时一般在几毫秒左右；看代码在获取特征的地方限制了300ms超时，正常情况下应该不会超过这个时间的，除非是有什么操作导致了获取数据得去读磁盘。\n探究原因 初探 基于以上的想法，首先排查看是否在产生超时的那个时间内，磁盘的读会比正常情况大，下面是服务的某个实例机的I/O监控：\n基于上面两张监控图可以看到，确实在异常的时间点，磁盘的读会比其他时间段大很多，最大耗时到了 1s 以上，这种情况已经远超过设置的 300ms 超时时间了。此外，在这里也注意到了磁盘的写入速度也异常的大，峰值达到了287.9MB/s。观察到这里，猜测可能是磁盘的写过大导致读响应慢了（个人直觉-。-，具体未去验证），下面就是找出具体是什么原因导致那段时间内磁盘写入速度辣么大。\n再探 服务内部获取特征使用到的本地存储框架，是基于Rocksdb存储开发的，下面是该框架与服务的简单交互流程：\n由一个索引服务去同步特征数据写成Rocksdb表，再将写好的Rocksdb文件打包压缩（tar文件）上传到Hdfs上\n排序服务从Hdfs上下载索引服务上传的tar文件到本地，再进行解包解压，然后恢复成Rocksdb表提供查询\n根据这个流程，再结合服务内部的一些日志，查到了在异常发生的时间段内会有表索引下载操作，所以认为可能是下载Hdfs文件到本地这一操作导致磁盘写入过快，因此在下载那里做了一个限速尝试，但上线后依然写入没有下降。后来再仔细对比了下载日志与磁盘写入监控，发现磁盘写入飙升的时间点刚好是tar文件刚下载完成后，所以猜测可能是tar文件的解压所导致的，接着查看了代码内tar文件的解压方式，发现内部是采用Linux的tar命令去做解压的，再从网上查到tar命令确实会有解压速度的问题。\n解决方法 解决tar命令解压写入速度过快的方法，从网上找到了一种方法说可以结合pv命令使用，但因为考虑安装服务的机器可能没有pv命令，所以没有采用这种方法。因考虑到用命令的方式无法控制写入速度，所以后面决定改用Java的方式去解压，这里采用了Apache的commons-compress里面的TarArchiveInputStream去做解压，具体实现如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 // FileUtils.java public static void unTar(InputStream in, String outputDir) throws IOException { unTar(in, outputDir, IOUtils::writeWithoutLimit); } private static void unTar(InputStream in, String outputDir, IOBiConsumer\u003cFile, InputStream\u003e writeFileOp) throws IOException { try (TarArchiveInputStream tis = new TarArchiveInputStream(in)) { TarArchiveEntry nextEntry; while ((nextEntry = tis.getNextTarEntry()) != null) { if (nextEntry.isDirectory()) { continue; } writeFileOp.accept(new File(outputDir, nextEntry.getName()), tis); } } } // IOUtils.java public static void writeWithoutLimit(File outputFile, InputStream in) throws IOException { createParentIfNotExists(outputFile); try (BufferedOutputStream bos = new BufferedOutputStream(new FileOutputStream(outputFile))) { writeWithoutLimit(bos, in); } } private static void createParentIfNotExists(File outputFile) { if (!outputFile.getParentFile().exists()) { outputFile.getParentFile().mkdirs(); } } public static void writeWithoutLimit(OutputStream out, InputStream in) throws IOException { byte[] bytes = new byte[1024]; int len; while ((len = in.read(bytes)) \u003e 0) { out.write(bytes, 0, len); } out.flush(); } 更换了解压方式后，磁盘写入速度峰值降到了90MB/s左右，磁盘读耗时峰值也降到了300ms，下面是更换后的I/O监控：\n异常超时的量也从原来的1600多个，降到了200多个，下面是更换后的异常超时监控：\n后面发现存储框架那边的流程其实可以简化，让排序服务直接读取Hdfs上的tar文件并解压到本地，可以省去从Hdfs上下载文件到本地的过程，不仅减少了索引下载到加载的时间，而且也减少了磁盘的写入，修改后的磁盘写入速度下降到了50MB/s以下，磁盘读取耗时峰值降到了170ms左右，异常超时发生的数量也减了不少。\n总结 本次所遇到的问题算是比较不常见的，因为日常做业务开发涉及到磁盘I/O的可能会比较少，因此也很少关注这方面的问题；刚好是服务用到了本地存储这个对磁盘读写比较敏感的东西，磁盘有一点比较大的波动就可能对服务性能产生影响，因此需要特别留意磁盘的写入速度。这次排查能够比较顺利很大的功劳在于监控系统，在业务监控系统能够看到异常发生的问题点以及时间点，在基础监控系统能够看到机器磁盘I/O的变化情况，再结合部分业务日志，定位问题显然高效了许多，所以有一套比较完善的监控体系真挺重要的。\n","description":"","tags":["tar命令","磁盘I/O"],"title":"采坑系列-tar命令解压写入速度过快","uri":"/posts/2022-03-09-tar_command_write_too_fast/"},{"categories":["采坑系列"],"content":" 采坑系列是记录日常学习/工作中所遇到的问题，可能是一个Bug、一次性能优化、一次思考等，目的是记录自己所处理过的问题，以及解决问题这一过程中所做的思考或总结，避免后续再犯相似的错误。\n问题描述 在并发场景下ArrayList是线程非安全的，并发往ArrayList里面添加元素，可能导致内部出现Null值的情况，尽管你添加进去的元素能保证非Null，但Null值不是来源你添加进去的元素，而是因为并发add导致ArrayList内部索引错乱，下面例子可复现错误（如果跑一次没有复现需跑多几次）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 public static void main(String[] args) throws Exception { List\u003cInteger\u003e arr = new ArrayList\u003c\u003e(); int threadNum = 10; CountDownLatch await = new CountDownLatch(threadNum); CyclicBarrier barrier = new CyclicBarrier(threadNum); for (int i = 0; i \u003c threadNum; i++) { int finalI = i; new Thread(() -\u003e { try { barrier.await(); } catch (InterruptedException | BrokenBarrierException e) { e.printStackTrace(); } arr.add(finalI); await.countDown(); }).start(); } await.await(); System.out.println(\"arr.size=\" + arr.size()); for (int i = 0; i \u003c arr.size(); i++) { if (arr.get(i) == null) { System.out.println(\"Value is null, index=\" + i); } } } 复现错误的输出如下，可见index为0的位置出现了Null值：\nDebug断点后可看到如下arr变量的数据，发现在index为0的位置确实是Null值，而且在arr里面缺少了本应该存在的数值0，说明在并发情况下ArrayList发生了不可预知的错误。\n探究原因 初探 查看ArrayList的add方法源码，内部操作很简单，只有三行代码：\nensureCapacityInternal，内部对modCount值加1，并判断添加元素是否需要扩容，需要则对底层数组进行扩容\nelementData[size++] = e，将待添加元素e存放进数组，然后执行size++\nreturn true，返回添加成功\n1 2 3 4 5 public boolean add(E e) { ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true; } 从上面可看到修改ArrayList元素的地方是第二行代码elementData[size++] = e;，该操作包含了size++还有赋值操作，我们知道自增操作是非原子性的，可拆分为：读-加-写，那么这一语句就包含了4个操作：读-加-写-数组元素赋值，类似于下面的操作（下面的代码只是做说明，实际执行的是字节码指令）：\n拆分出上面几个操作后，我们可以尝试分析下在多个线程同时执行上面操作时，会出现哪些问题\n覆盖/丢失值 线程1拿到的size为0，执行完操作2后让出CPU给到线程2，此时index为0，size未更新还是0\n线程2开始执行拿到的size为0，index为0，执行完以上所有操作后让出CPU给到线程1，此时size被更新为1\n线程1继续执行步骤3和4，index为之前获取到的值0，size被更新为1，因此出现线程1的元素覆盖了线程2的元素（线程2的元素丢失了）\n到这里貌似没能找出为啥会出现Null值的原因，反而分析出了另外一个问题-。-；既然Null值问题不是出现在第二行代码，那么就只有可能在第一行代码里面出现了（废话-__-）\n再探 回头看ensureCapacityInternal方法里面做了什么事情：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 private void ensureCapacityInternal(int minCapacity) { ensureExplicitCapacity(calculateCapacity(elementData, minCapacity)); } private static int calculateCapacity(Object[] elementData, int minCapacity) { // 如果是空数组，则返回DEFAULT_CAPACITY（默认是10）与要求的最少容量minCapacity中的最大者 if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) { return Math.max(DEFAULT_CAPACITY, minCapacity); } return minCapacity; } private void ensureExplicitCapacity(int minCapacity) { modCount++; // overflow-conscious code if (minCapacity - elementData.length \u003e 0) grow(minCapacity); } private void grow(int minCapacity) { // overflow-conscious code int oldCapacity = elementData.length; // 默认扩大原数组长度的1/2倍 int newCapacity = oldCapacity + (oldCapacity \u003e\u003e 1); // 不满足所要求的容量则以要求的容量为准 if (newCapacity - minCapacity \u003c 0) newCapacity = minCapacity; // 如果新容量大于期望的最大容量，确认最终使用的容量（MAX_ARRAY_SIZE or Integer.MAX_VALUE) if (newCapacity - MAX_ARRAY_SIZE \u003e 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity); } private static int hugeCapacity(int minCapacity) { if (minCapacity \u003c 0) // overflow throw new OutOfMemoryError(); return (minCapacity \u003e MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; } calculateCapacity：计算数组所需要的容量\nensureExplicitCapacity：修改modCount值，确认是否数组所需要的容量大于现有数组的长度，是则表明需要对数组进行扩容\ngrow：对数组进行扩容，默认扩大原数组长度的1/2倍，如果扩大后还不满足所要求的容量minCapacity，则以要求的容量为准，如果新容量newCapacity大于MAX_ARRAY_SIZE，则取MAX_ARRAY_SIZE或Integer.MAX_VALUE（取决于minCapacity是否大于MAX_ARRAY_SIZE）\nPS：MAX_ARRAY_SIZE值为Integer.MAX_VALUE - 8，为啥减8可自行查看MAX_ARRAY_SIZE的Java doc\n从上面代码来看，会修改数组的只有扩容方法里面的这一句：elementData = Arrays.copyOf(elementData, newCapacity)，这里会将原数组数据复制到扩大长度后的新数组，并将数组引用指向新数组，考虑以下的并发情况：\n线程1拿到的size为0，要插入元素需要进行扩容，还未执行elementData = Arrays.copyOf(elementData, newCapacity);时让出了CPU给到线程2\n线程2拿到的size也为0（因线程1还未执行到elementData[size++] = e;），要插入元素需要进行扩容，扩容完成后往下标为0的位置设置了元素并成功返回，此时size变成了1\n线程1继续执行扩容，拿到的数组可能还是旧的elementDate（根据JVM的内存模型，每个线程在使用主内存中的变量时，会将变量拷贝到自己的工作内存中，线程1读取elementDate时可能线程2对elementDate的修改还未写回主内存，所以拿到的是旧值），线程1扩容后的新数组（数据全部为null）将赋值给elementDate，之后线程1将执行elementData[size++] = e;，此时获取到的size可能为1（由线程1对size的修改被线程2读到了），即会给下标为1的位置赋值，然后size变成了2，至此导致下标为0的位置出现了null值\n解决方法 如果必须在并发情况下使用List，可按以下方式解决List并发安全问题，可根据实际业务场景选用：\n使用线程安全的java.util.Vector类（具体是对每个方法加synchronized保证线程安全，不推荐使用）\n使用java.util.Collections类的synchronizedList方法对List进行包装，让其变成线程安全的类（具体是对非线程安全的方法内部加了synchronized保证线程安全，推荐使用）\n使用JUC的java.util.concurrent.CopyOnWriteArrayList类（写操作加锁且数组变量用volatile修饰，读操作无锁，具体是在写内部使用ReentrantLock加锁，并且采用复制原数组-往新数组添加元素-替换旧数组的形式，保证读操作的线程安全，适合读多写少的场景）\n总结 本次针对ArrayList并发写出现Null值的问题进行探究，最初怀疑是因为size++不是原子性导致的（而且貌似网上也挺多这样说的-_-），按照思路分析下去后才发现问题不是出现在这里，不过也发现了size++不是原子性导致的另外一个并发问题（覆盖/丢失值）；后面回过头来分析扩容操作，看到针对实例变量的赋值后，有点想不起一个线程对实例变量修改后，另一线程用到这一实例变量会怎么样了，又去看了下JVM内存模型相关的资料，最后才确认了问题的根源。现在想想一个小问题涉及到的知识点其实蛮多的，做下复盘其实也是一次很好的学习机会。\n","description":"","tags":["ArrayList","并发问题"],"title":"采坑系列-ArrayList并发写出现Null值","uri":"/posts/2022-03-03-arraylist_concurrency-_issues/"},{"categories":["工具"],"content":"安装 JMeter安装 JMeter的安装很简单，直接访问 官方下载页面 下载即可。\nJMeter Plugins Manager安装 JMeter的插件管理安装不是必须的，不过安装插件管理后可以下载一些有用的插件，建议也安装一下。\n启动JMeter后，可以通过Options → Plugins Manager 选项打开插件管理界面。\n插件管理界面可以在Installed Plugins栏看到已安装的插件，如果将已安装的插件去掉勾选，则表示要卸载插件；在Available Plugins栏可以看到可用的插件，可以勾选需要安装的插件，然后点击右下方的Apply Changes and Restart JMeter按钮保存即可。\n启动 JMeter的启动也很简单，只需要找到安装目录下的 bin/jmeter.bat（Windows系统）或 bin/jmeter.sh （Mac或Linux系统）执行即可，这种方式或启动JMeter的GUI界面。\n使用 下面以配置固定吞吐量10/s来测试HTTP接口为例说明如何使用。\n配置线程组 首先需要在测试计划上点击右键 Add → Threads(Users) → Thread Group 添加线程组，线程组用来模拟访问测试接口的用户。\n线程组涉及了如下的一些配置：\nAction to be taken after a Sampler error：由于采样器失败或断言失败而导致执行遇到错误时该如何处理\nContinue：忽略错误，继续执行\nStart Next Thread Loop：忽略错误，终止当前线程的循环，执行下一个循环\nStop Thread：终止当前执行的线程，不影响其他线程的执行\nStop Test：在当前正在执行的线程执行完后停止测试\nStop Test Now：立即终止当前正在执行的线程后停止测试\nNumber of Threads(users)：定义执行时想模拟的线程（用户）数量\nRamp-up period(seconds)：全部线程启动执行的时间，比如线程数量设置为10，启动执行时间设置为100秒，那么第一个线程将在第0秒（测试执行开始时）启动执行，然后每个线程将在10秒(100/10)后启动执行\nLoop Count：每个线程的执行次数，可以设置为不限次数（Infinite）或固定次数，比如循环数设置为2，线程数为100，那么总的将运行200次\nSame user on each iteration：在每次循环中使用相同的用户，当有设置HTTP Cookie Manager时，开始这个选项后会将在第一个响应中获得的cookie用于后续的请求\nDelay Thread creation until needed：延迟创建线程所需的资源，比如设置要启动1000个线程，即使设置了Ramp-up，JMeter也会立即为所有线程分配内存；启用该选项则可以在新线程开始执行时分配内存\nSpecify Thread lifetime\nDuration(seconds)：设置线程组运行多长时间，跟线程数和每个线程运行次数配合使用，即使线程数和线程运行次数还未跑完，持续时间到了，线程组直接停止\nStartup delay(seconds)：设置线程启动延时时间，是指多少秒后才开始启动线程，不是说每个线程都延迟多少秒启动\n配置HTTP采样器 在线程组上点击右键 Add → Sampler→Http Request添加HTTP采样器，一般测试GET方法或简单的POST方法基本上用这个采样器就足够了，但有时候我们需要设置一些请求头信息，可以看到在HTTP采样器这个里面是没有办法配置的，这时候就需要用到配置元素HTTP Header Manager。\n配置HTTP头管理器 在线程组上点击右键 Add → Sampler→HTTP Header Manager添加HTTP请求头管理器，点击下方的Add可以添加请求头配置项，如下添加了Content-Type: application/json。\n配置固定吞吐量计时器 在线程组上点击右键 Add → Timer→Constant Throughput Timer添加固定吞吐量计时器，该计时器可以控制请求执行的吞吐量，下面设置了线程组内的所有线程执行吞吐量控制在600/min（10/s）左右（可能会有误差），当然这个的前提是要线程组的执行次数能满足要求。\n配置聚合报告 在线程组上点击右键 Add → Listener→Aggregate Report添加聚合报告，聚合报告可以查看样本执行的耗时、吞吐量和接发数据量情况，如下是控制吞吐量在10/s左右执行1000次的聚合结果。\n聚合报告中每一行（除了最后一行）为每个HTTP采样器的执行情况，最后一行为统计所有采样器的执行情况，每一列表示的意思如下说明：\nLabel：HTTP采样器设置的名称\n#Samples：总请求样本数\nAverage：平均耗时，单位ms\nMedian：中位数耗时，单位ms\n90%Line：90分位线耗时，单位ms\n95%Line：95分位线耗时，单位ms\n99%Line：99分位线耗时，单位ms\nMin：最小耗时，单位ms\nMaximum：最大耗时，单位ms\nError%：错误百分比\nThroughput：每秒处理的请求数（n/sec）\nReceived KB/sec：每秒接收的数据量（KB/sec）\nSent KB/sec：每秒发送的数据量（KB/sec）\nLinux下执行JMeter 在本地配置好要执行的测试计划后保存成脚本xxx.jmx（自由命名），然后将文件上传到Linux服务器上，然后输入以下命令执行（以下~/jmeter/apache-jmeter-5.4.1为服务器上JMeter的安装目录）。\n1 ~/jmeter/apache-jmeter-5.4.1/bin/jmeter -n -t ~/jmeter/xxx.jmx -l ~/jmeter/xxx.jtl JMeter启动常用参数说明：\n1 2 3 4 5 6 7 -h 帮助：打印出有用的信息并退出 -n 非 GUI 模式：在非 GUI 模式下运行 JMeter -t 测试文件：要运行的 JMeter 测试脚本文件 -l 输出文件：记录结果的文件 -r 远程执行：启动远程服务 -H 代理主机：设置 JMeter 使用的代理主机 -P 代理端口：设置 JMeter 使用的代理主机的端口号 测试计划执行完成后会输出xxx.jtl文件，将其下载到本地，然后启动JMeter，添加所需的监听器并打开浏览即可。\n总结 本文首先介绍了如何安装JMeter，安装步骤很简单，直接上官网下载安装包安装即可；接着通过一个简单的HTTP请求接口测试例子介绍了如何使用JMeter，并在其中对一些关键的配置进行说明；最后介绍了如何在Linux上运行JMeter，通过非GUI模式启动脚本运行并输出执行结果文件，再通过GUI界面打开文件浏览即可。\n参考 5 must know features of Thread Group in Jmeter\nGuide to JMeter Thread Groups\nIntroducing JMeter 5.2!\nHow to Use the Delay Thread Creation on JMeter\nUnderstand and Analyze Aggregate Report in Jmeter\nlinux环境运行jmeter并生成报告\n","description":"","tags":["工具","压测","JMeter"],"title":"JMeter压测工具使用入门","uri":"/posts/2022-01-08-jmeter_induction/"},{"categories":["Spring"],"content":" 每日一言：知识是勤奋的影子，汗珠是勤奋的镜子\n配置多数据源 maven依赖 1 2 3 4 5 6 7 8 9 10 11 12 \u003cparent\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-parent\u003c/artifactId\u003e \u003cversion\u003e2.1.0.RELEASE\u003c/version\u003e \u003crelativePath/\u003e \u003c!-- lookup parent from repository --\u003e \u003c/parent\u003e \u003cdependencies\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-data-jpa\u003c/artifactId\u003e \u003c/dependency\u003e \u003c/dependencies\u003e application.yml文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 spring: datasource: primary: name: primaryDataSource driver-class-name: org.h2.Driver url: jdbc:h2:mem:test1;MODE=MySQL username: root password: root jpa: show-sql: true generate-ddl: true properties: hibernate.dialect: org.hibernate.dialect.H2Dialect hibernate: ddl-auto: update secondary: name: secondaryDataSource driver-class-name: org.h2.Driver url: jdbc:h2:mem:test2;MODE=MySQL username: root password: root jpa: show-sql: true generate-ddl: true properties: hibernate.dialect: org.hibernate.dialect.H2Dialect hibernate: ddl-auto: update Java Config 第一个数据源配置类\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 @Configuration @EnableJpaRepositories( entityManagerFactoryRef = \"primaryEntityManager\", transactionManagerRef = \"primaryTransactionManager\", basePackageClasses = {com.horizon.article.demo.jpa.primary.repo.Package.class}) public class PrimaryDsConfiguration { @Bean(\"primaryDataSourceProperties\") @ConfigurationProperties(prefix = \"spring.datasource.primary\") public DataSourceProperties primaryDataSourceProperties() { return new DataSourceProperties(); } @Bean(\"primaryDataSource\") public DataSource primaryDataSource() { return primaryDataSourceProperties().initializeDataSourceBuilder().build(); } @Bean(\"primaryJpaProperties\") @ConfigurationProperties(prefix = \"spring.datasource.primary.jpa\") public JpaProperties primaryJpaProperties() { return new JpaProperties(); } @Bean(\"primaryHibernateProperties\") @ConfigurationProperties(prefix = \"spring.datasource.primary.jpa.hibernate\") public HibernateProperties primaryHibernateProperties() { return new HibernateProperties(); } @Bean(\"primaryEntityManager\") public LocalContainerEntityManagerFactoryBean primaryEntityManager() { final LocalContainerEntityManagerFactoryBean em = new LocalContainerEntityManagerFactoryBean(); em.setPackagesToScan(com.horizon.article.demo.jpa.primary.bean.Package.class.getPackage().getName() ); em.setDataSource(primaryDataSource()); em.setJpaVendorAdapter(primaryVendorAdapter()); Map\u003cString, Object\u003e properties = primaryHibernateProperties() .determineHibernateProperties( primaryJpaProperties().getProperties(), new HibernateSettings() ); em.setJpaPropertyMap(properties); em.setPersistenceUnitName(\"primary\"); return em; } @Bean(\"primaryVendorAdapter\") public HibernateJpaVendorAdapter primaryVendorAdapter() { final HibernateJpaVendorAdapter adapter = new HibernateJpaVendorAdapter(); adapter.setGenerateDdl(false); return adapter; } @Bean(\"primaryTransactionManager\") public PlatformTransactionManager primaryTransactionManager() { final JpaTransactionManager transactionManager = new JpaTransactionManager(); transactionManager.setEntityManagerFactory(primaryEntityManager().getObject()); return transactionManager; } @Bean @Qualifier(\"primaryJdbcTemplate\") public JdbcTemplate primaryJdbcTemplate() { return new JdbcTemplate(primaryDataSource()); } } 以同样的方式配置第二个数据源\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 @Configuration @EnableJpaRepositories( entityManagerFactoryRef = \"secondaryEntityManager\", transactionManagerRef = \"secondaryTransactionManager\", basePackageClasses = {com.horizon.article.demo.jpa.secondary.repo.Package.class}) public class SecondaryDsConfiguration { @Bean(\"secondaryDataSourceProperties\") @ConfigurationProperties(prefix = \"spring.datasource.secondary\") public DataSourceProperties secondaryDataSourceProperties() { return new DataSourceProperties(); } @Bean(\"secondaryDataSource\") public DataSource secondaryDataSource() { return secondaryDataSourceProperties().initializeDataSourceBuilder().build(); } @Bean(\"secondaryJpaProperties\") @ConfigurationProperties(prefix = \"spring.datasource.secondary.jpa\") public JpaProperties secondaryJpaProperties() { return new JpaProperties(); } @Bean(\"secondaryHibernateProperties\") @ConfigurationProperties(prefix = \"spring.datasource.secondary.jpa.hibernate\") public HibernateProperties secondaryHibernateProperties() { return new HibernateProperties(); } @Bean(\"secondaryEntityManager\") public LocalContainerEntityManagerFactoryBean secondaryEntityManager() { final LocalContainerEntityManagerFactoryBean em = new LocalContainerEntityManagerFactoryBean(); em.setPackagesToScan(com.horizon.article.demo.jpa.secondary.bean.Package.class.getPackage().getName() ); em.setDataSource(secondaryDataSource()); em.setJpaVendorAdapter(secondaryVendorAdapter()); Map\u003cString, Object\u003e properties = secondaryHibernateProperties() .determineHibernateProperties( secondaryJpaProperties().getProperties(), new HibernateSettings() ); em.setJpaPropertyMap(properties); em.setPersistenceUnitName(\"secondary\"); return em; } @Bean(\"secondaryVendorAdapter\") public HibernateJpaVendorAdapter secondaryVendorAdapter() { final HibernateJpaVendorAdapter adapter = new HibernateJpaVendorAdapter(); adapter.setGenerateDdl(false); return adapter; } @Bean(\"secondaryTransactionManager\") public PlatformTransactionManager secondaryTransactionManager() { final JpaTransactionManager transactionManager = new JpaTransactionManager(); transactionManager.setEntityManagerFactory(secondaryEntityManager().getObject()); return transactionManager; } @Bean @Qualifier(\"secondaryJdbcTemplate\") public JdbcTemplate secondaryJdbcTemplate() { return new JdbcTemplate(secondaryDataSource()); } } 排除SpringBoot本身的数据源自动配置及JPA自动配置\n1 2 3 4 5 6 7 8 9 10 11 12 13 @SpringBootApplication(exclude = { DataSourceAutoConfiguration.class, DataSourceTransactionManagerAutoConfiguration.class, JdbcTemplateAutoConfiguration.class, HibernateJpaAutoConfiguration.class }) public class Application { public static void main(String[] args) { SpringApplication.run(Application.class, args); } } 完整示例代码地址\n配置动态切换数据源 使用枚举（或字符串）定义不同数据源的唯一标识 1 2 3 public enum DatabaseEnvironment { DEVELOPMENT, TESTING, PRODUCTION } 使用ThreadLocal存放当前线程持有的环境 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 public class DatabaseContextHolder { private static final ThreadLocal\u003cDatabaseEnvironment\u003e CONTEXT = new ThreadLocal\u003c\u003e(); public static void set(DatabaseEnvironment databaseEnvironment) { CONTEXT.set(databaseEnvironment); } public static DatabaseEnvironment getEnvironment() { return CONTEXT.get(); } public static void clear() { CONTEXT.remove(); } } 继承AbstractRoutingDataSource类，创建自定义路由数据源 1 2 3 4 5 6 7 public class DataSourceRouter extends AbstractRoutingDataSource { @Override protected Object determineCurrentLookupKey() { return DatabaseContextHolder.getEnvironment(); } } Java Config配置路由数据源 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 @Configuration @EnableJpaRepositories( basePackageClasses = CustomerRepository.class, entityManagerFactoryRef = \"customerEntityManager\", transactionManagerRef = \"customerTransactionManager\") public class DataSourceConfiguration { @Autowired(required = false) private PersistenceUnitManager persistenceUnitManager; @Bean @ConfigurationProperties(\"app.customer.jpa\") public JpaProperties customerJpaProperties() { return new JpaProperties(); } @Bean @ConfigurationProperties(\"app.customer.jpa.hibernate\") public HibernateProperties customerHibernateProperties() { return new HibernateProperties(); } @Bean @ConfigurationProperties(\"app.customer.development.datasource\") public DataSourceProperties developmentDataSourceProperties() { return new DataSourceProperties(); } @Bean public DataSource developmentDataSource() { return developmentDataSourceProperties().initializeDataSourceBuilder().build(); } @Bean @ConfigurationProperties(\"app.customer.testing.datasource\") public DataSourceProperties testingDataSourceProperties() { return new DataSourceProperties(); } @Bean public DataSource testingDataSource() { return testingDataSourceProperties().initializeDataSourceBuilder().build(); } @Bean @ConfigurationProperties(\"app.customer.production.datasource\") public DataSourceProperties productionDataSourceProperties() { return new DataSourceProperties(); } @Bean public DataSource productionDataSource() { return testingDataSourceProperties().initializeDataSourceBuilder().build(); } /** * Adds all available datasources to datasource map. * * @return datasource of current context */ @Bean @Primary public DataSource customerDataSource() { DataSourceRouter router = new DataSourceRouter(); final HashMap\u003cObject, Object\u003e map = new HashMap\u003c\u003e(3); map.put(DatabaseEnvironment.DEVELOPMENT, developmentDataSource()); map.put(DatabaseEnvironment.TESTING, testingDataSource()); map.put(DatabaseEnvironment.PRODUCTION, productionDataSource()); router.setTargetDataSources(map); return router; } @Bean public LocalContainerEntityManagerFactoryBean customerEntityManager( @Qualifier(\"customerJpaProperties\") final JpaProperties customerJpaProperties) { EntityManagerFactoryBuilder builder = createEntityManagerFactoryBuilder(customerJpaProperties); return builder.dataSource(customerDataSource()).packages(Customer.class) .properties(customerHibernateProperties().determineHibernateProperties( customerJpaProperties.getProperties(), new HibernateSettings() )).persistenceUnit(\"customerEntityManager\").build(); } @Bean @Primary public JpaTransactionManager customerTransactionManager( @Qualifier(\"customerEntityManager\") final EntityManagerFactory factory) { return new JpaTransactionManager(factory); } private EntityManagerFactoryBuilder createEntityManagerFactoryBuilder( final JpaProperties customerJpaProperties) { JpaVendorAdapter jpaVendorAdapter = createJpaVendorAdapter(customerJpaProperties); return new EntityManagerFactoryBuilder(jpaVendorAdapter, customerJpaProperties.getProperties(), persistenceUnitManager); } private JpaVendorAdapter createJpaVendorAdapter( JpaProperties jpaProperties) { AbstractJpaVendorAdapter adapter = new HibernateJpaVendorAdapter(); adapter.setShowSql(jpaProperties.isShowSql()); adapter.setGenerateDdl(jpaProperties.isGenerateDdl()); return adapter; } } 定义AOP拦截切换不同数据源 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 @Aspect @Component public class DataSourceDetermineAspect { @Before(\"@annotation(applyDataSource)\") public void before(ApplyDataSource applyDataSource) { DatabaseContextHolder.set(applyDataSource.value()); } @After(\"@annotation(applyDataSource)\") public void after(ApplyDataSource applyDataSource) { DatabaseContextHolder.clear(); } } ","description":"","tags":["JPA","多数据源"],"title":"Spring Data JPA多数据源配置","uri":"/posts/2021-10-28-jpa_multi_data_source/"},{"categories":["前端"],"content":"Promise是什么？ 理解 Promise是ES6引入的一种异步编程的解决方案 具体表达 从语法上来说，Promise是一个构造函数 从功能上来说，promise对象用来封装一个异步操作并可以获取其返回结果 promise的状态改变 pending 变为 resolved（fulfilled） pending 变为 rejected PS：一个promise对象只能改变一次，无论变为成功或失败，都会有一个结果数据，成功的结果一般称为 value，失败的结果一般称为 reason promise的基本运行流程 promise的基本使用 const p = new Promise((resolve, reject) =\u003e { setTimeout(() =\u003e { const time = Date.now(); if (time % 2 === 0) { resolve('成功的数据，time=' + time) } else { reject('失败的数据，time=' + time) } }, 1000) }) p.then(value =\u003e { console.log('成功的回调', value) }, reason =\u003e { console.log('失败的回调', value) }) 为什么要用Promise 指定回调函数的方式更加灵活 旧的：必须在启动异步任务前指定 promise：启动异步任务 =\u003e 返回promise对象 =\u003e 给promise对象绑定回调函数（甚至可以在异步任务执行结束后再指定） 支持链式调用，解决回调地狱问题 什么是回调地狱？ 回调函数嵌套调用，外部回调函数异步执行的结果是嵌套的回调函数执行的条件 回调地狱的缺点？ 不便于阅读/不便于异常处理 解决方案：promise链式调用 终极解决方案：async/await 如何使用Promise？ API Promise构造函数：Promise(executor) executor函数：执行器 (resolve, reject) =\u003e {} resolve函数：成功时调用的函数 resolve =\u003e {} reject函数：失败时调用的函数 reject =\u003e {} Promise.prototype.then Promise.prototype.catch Promise.prototype.finally Promise.resolve Promise.reject Promise.race Promise.all …. promise的几个关键问题 如何改变promise的状态？ resolve(vale)：如果当前是 pending 就会变为 resolved reject(reason)：如果当前是 pending 就会变为 rejected 抛出异常：如果当前是 pending 就会变为 rejected 一个 promise 指定多个成功/失败回调函数，都会调用吗？ 单 promise 改变为对应状态时都会调用\n改变 promise 状态和指定回调函数谁先谁后？\n都有可能，正常情况下是先指定回调再改变状态，但也可以先改状态再指定回调 如何先改状态再指定回调？ 在执行器中直接调用resolve()/reject() 延迟更长时间才调用then() promise.then() 返回的新 promise 的结果状态由什么决定？ 简单表达：由 then() 指定的回调函数执行的结果决定 详细表达： 如果抛出异常，新 promise 变为 rejected，reason 为抛出的异常 如果返回的是非 promise 对象，新 promise 变为 resolved，value 为返回的值 如果返回的是一个 promise 对象，此 promise 对象的结果会成为新 promise 的结果 promise 如何串连多个操作任务？ promise 的 then() 返回一个新的 promise，通过 then() 的链式调用串连多个同步/异步任务\npromise 异常传透\n理解：当使用 promise 的 then 链式调用时，可以在最后指定失败的回调，前面任何操作出了异常，都会传到最后失败的回调中处理 原理：then 方法未指定 onRejected 回调函数时，其默认值会是：reason =\u003e Promise.reject(reason)，将异常传递下去 中断 promise 链 理解：当使用 promise 的 then 链式调用时，在中间中断，不在调用后面的回调函数 办法：在回调函数中返回一个 pending 状态的 promise 对象（如：new Promise(() =\u003e {})） async与await async 函数 函数的返回值是 promise 对象 promise 对象的结果由 async 函数执行的返回值决定 await 表达式 await 右侧表达式一般为 promise 对象，但也可以是其他值 如果表达式是 promise 对象，await 返回的是 promise 成功的值 如果表达式是其他值，直接将此值作为 await 的返回值 注意点 await 必须写在 async 函数中，但 async 函数可以没有 await 如果 await 的 promise 失败了，会抛出异常，需要通过try…catch捕获处理 JS异步之宏队列与微队列 原理图 图片来源：https://www.cnblogs.com/BAHG/p/12921321.html 说明 JS中用来存储执行回调函数的队列包含2个，分别为宏队列和微队列 宏队列：用来保存待执行的宏任务（回调），比如：定时器回调/DOM事件回调/Ajax回调 微队列：用来保存待执行的微任务（回调），比如：promise的回调、MutationObserver的回调 JS执行时会区分这两个队列 JS引擎首先必须先执行所有的初始化同步任务代码 每次准备取出第一个宏任务执行前，都要将所有的微任务取出执行，即微队列的优先级比宏任务高，且与微任务所处的代码位置无关 ","description":"","tags":["Promise"],"title":"Promise学习笔记（B站）","uri":"/posts/2021-04-18-atguigu_promise_note/"},{"categories":["前端"],"content":" 此文为学习 尚硅谷Web前端ES6教程，涵盖ES6-ES11 记录的笔记\nES6 let变量声明 变量不能重复声明 let i = 1; let i = 2; // error 块级作用域 { let i = 1; } console.log(i); // error 不存在变量提升 console.log(i); // error let i = 1; 不影响作用域链 { let i = 1; function fn() { console.log(i); // 1 } fn(); } const声明常量 常量声明时需要赋初始值，一般常量名使用大写 const ERROR; // error 常量值不能修改 const ERROR = \"error\"; ERROR = \"\"; // error 块级作用域（同let） 对于数组和对象值的修改，不算对常量的修改 const ARR = [\"a\", \"b\"]; ARR[0] = \"c\"; ARR.push(\"d\"); const OBJ = { name: \"a\" } OBJ[name] = \"b\"; OBJ.age = 10; 变量的解构赋值 数组解构 let arr = [\"a\", \"b\"]; const [a, b] = arr; 对象解构 let obj = { name: 'horizon', age: 23 }; const {name, age} = obj; 模板字符串 // 使用 `` 定义模板字符串 const key = \"key\" const str = `${key}-value` 对象的简化写法 const name = \"horizon\"; const age = 23; const user = { name, age eat() { console.log('eat'); } }; 箭头函数 this是静态的。this 始终指向函数声明时所在作用域下的 this 的值 function getName() { console.log(this.name); } const getName2 = () =\u003e { console.log(this.name); } window.name = 'horizon'; getName(); // horizon getName2(); // horizon const user = { name: 'jeson' } getName.call(user); // jeson getName2.call(user); // horizon 不能作为构造实例化对象 const Persion = (name) =\u003e { this.name = name; } const me = new Persion('horizon'); // error 不能使用 arguments 变量 const fn = () =\u003e { console.log(arguments); // error } 箭头函数的简写 // 1. 当形参有且只有一个时，可以省略小括号 const fn = n =\u003e { return n + n; } // 2. 当代码体只有一条语句时，可以省略花括号，且如果该语句是返回值的语句，必须省略 return const pw = n =\u003e n * n; 箭头函数的使用场景 适合与 this 无关的回调，如定时器、数组的方法回调 不适合与 this 有关的回调，如事件回调，对象的方法 函数参数的初始值，一般位置要靠后 形参初始值， function incr(a, b = 1) { return a + b; } console.log(incr(10)); // 11 与解构赋值结合 function getName({name = 'horizon'}) { console.log(name); } getName(); // horizon getName({name: 'jeson'}); // jeson rest参数 rest参数用于获取函数的实参，用来代替 arguments\n// arguments是对象 function output() { console.log(arguments); } // args是数组，必须放在参数的最后一个位置 const output2 = (...args) { console.log(args); } 扩展运算符 […] 扩展运算符能够将【数组】转换为逗号分隔的【参数序列】\nfunction output(a, b) { return a + b; } const arr = [1, 2]; console.log(output(...arr)); // 3 扩展运算符的应用 数组的合并 数组的克隆 将伪数组转为真正的数组 Symbol 新的原始数据类型，表示独一无二的值，Symbol的特点：\nSymbol的值是唯一的，用来解决命名冲突的问题 Symbol值不能与其他数据进行运算 Symbol定义的对象不能使用 for…in 循环符，但是可以使用 Reflect.ownKeys 来获取对象的所有键名 迭代器 迭代器是一种接口，为各种不同的数据结构提供统一的访问机制，任何数据结构只要部署了 Iterator 接口（对象里的一个属性：Symbol.iterator），就可以完成遍历操作\nES6创造了一种新的遍历命令 for…of 循环，Iterator 接口主要供 for…of 消费 原生具备 Iterator 接口的数据（可用 for…of 循环） Array Arguments Set Map String TypeArray NodeList 工作原理 创建一个指针对象，指向当前数据结构的起始位置 第一次调用对象的 next 方法，指针自动指向数据的第一个成员 接下来不断调用 next 方法，指针一直往后移动，直到指向最后一个 每调用 next 方法返回一个包含 value 和 done 属性的对象 const user = { name: 'horizon', skill: ['java', 'js', 'ts'], [Symbol.iterator]() { let i = 0; return { next: () =\u003e { if (i \u003c this.skill.length) { const result = { value: this.skill[i], done: false } i++; return result; } else { return { value: undefined, done: true } } } } } } for (const each of user) { console.log(each); } 生成器函数 生成器函数是ES6提供的一种异步编程解决方案，语法行为与传统函数完成不同\n使用 // 加\"*\"表明是生成器函数 function * gen() { console.log('1'); // code1 yield '1'; console.log('2'); // code2 yield '2'; } let iterator = gen(); // 调用生成器函数返回的是一个迭代器对象 iterator.next(); // 执行code1 iterator.next(); // 执行code2 // 输出 yield 返回的结果 for (const v of iterator) { console.log(v); } 实例 模拟获取：用户数据 -\u003e 订单数据 -\u003e 商品数据 // 调用生成器函数 let iterator = gen(); iterator.next(); function getUsers() { setTimeout(() =\u003e { let data = \"用户数据\"; iterator.next(data); }, 1000) } function getOrders() { setTimeout(() =\u003e { let data = \"订单数据\"; // 第二次调用 next 传入的实参将作为第一个 yield 语句返回的结果 iterator.next(data); }, 1000) } function getProducts() { setTimeout(() =\u003e { let data = \"商品数据\"; iterator.next(data); }, 1000) } function * gen() { const users = yield getUsers(); const orders = yield getOrders(); const products = yield getProducts(); } Promise Promise是ES6引入的异步编程的新解决方案，语法上Promise是一个构造函数，用来封装异步操作并可以获取成功或失败的结果。\nSet 常用API\nadd delete has clear size（属性） const set = new Set([1, 2, 3, 1]); for (const num of set) { console.log(num); } Map 常用API\nset delete get clear size（属性） let m = new Map(); m.set('name', 'horizon'); for (const o of m) { console.log(o); // 输出长度为2的数组，第一个元素为key，第二个元素为value } Class类 通过class关键字可以定义类，基本上ES6的class可以看做是一种语法糖，它的绝大部分功能ES5都可以做到，新的class写法只是让对象原型的写法更加清晰、更像面向对象编程的语法而已。\nclass声明类 constructor定义构造函数初始化 extends继承父类 super调用父类构造方法 static定义静态方法和属性 父类方法可以重写 // ES5实现类的写法 function Person(name, age) { this.name = name; this.age = age; } Person.prototype.eat = function() { console.log('eat'); } // ES6实现类的写法 class Person { constructor(name, age) { this.name = name; this.age = age; } eat() { console.log('eat'); } } Person person = new Person('horizon', 23); person.call(); // ES5实现继承 function Phone(brand, price) { this.brand = brand; this.price = price; } Phone.prototype.call = function() { console.log('I can call'); } function SmartPhone(brand, price, color) { Phone.call(this, brand, price); this.color = color } // 设置子级构造函数的原型 SmartPhone.prototype = new Phone; SmartPhone.prototype.contructor = SmartPhone SmartPhone.prototype.photo = function() { console.log('I can photo'); } // ES6实现继承 class Phone { constructor(brand, price) { this.brand = brand; this.price= price; } call() { console.log('I can call'); } } class SmartPhone extends Phone { constructor(brand, price, color) { super(brand, price); this.color = color; } photo() { console.log('I can photo'); } } 数值扩展 Math.EPSILON：js表示的最小精度 二进制和八进制 Number.isFinite：检测一个数值是否为有限数 Number.isNaN：检测一个数值是否为NaN Number.isInteger：判断一个数是否为整数 Math.trunc：将数字的小数部分抹掉 Math.sign：判断一个数是正数、零还是负数 对象方法扩展 Object.is：判断两个值是否相等 console.log(Object.is(1, 1)); // true console.log(NaN === NaN); // false console.log(Object.is(NaN, NaN)); // true Object.assign：对象的合并 Object.setPropertyOf、Object.getPropertyOf：设置原型对象 模块化 模块化是指将一个大的程序文件，拆分成许多小的文件，然后将小文件组合起来\n模块化的好处 防止命名冲突 代码复用 高维护性 模块化规范产品 CommonJS =\u003e NodeJS、Browserify AMD =\u003e requireJS CMD =\u003e seaJS ES6模块化语法 export命令用于规定模块的对外接口 import命令用于输入其他模块提供的功能 ES7 Array.prototype.includes includes 方法用来检测数组中是否包含某个元素，返回布尔类型值\n指数操作符 在ES7中引入了指数运算符[**]，用来实现幂运算，功能与Math.pow结果相同\nES8 async和await async和await两种语法结合可以让异步代码像同步代码一样\nasync函数 async函数的返回值为Promise对象 Promise对象的结果由async函数执行的返回值决定 await表达式 await必须写在async函数中 await右侧的表达式一般为Promise对象 await返回的是Promise成功的值 await的Promise失败了，就会抛出异常，需要通过try…catch捕获处理 对象方法扩展 Object.values()：返回一个给定对象的所有可枚举属性值的数组 Object.entries()：返回一个给定对象自身可遍历属性[key,value]的数组 Object.getOwnPropertyDescriptors()：返回指定对象所有自身属性的描述对象 ES9 扩展运算符与rest参数 扩展运算符与rest参数在ES6中已引入，但只针对数组，在ES9中为对象提供了像数组一样的扩展运算符与rest参数\n正则扩展 命名捕获分组（?） let str = '\u003ca href=\"http://www.baidu.com\"\u003e百度\u003c/a\u003e'; const reg = /\u003ca href=\"(?\u003curl\u003e.*)\"\u003e(?\u003ctext\u003e.*)\u003c\\/a\u003e/; const result = reg.exec(str) console.log(result.groups.url); console.log(result.groups.text); 反向断言（?\u003c=） let str = \"ES110哔哔120哩哩\"; // 正向断言 const reg = /\\d+(?=哩)/; console.log(reg.exec(str)); // 120 // 反向断言 const reg2 = /(?\u003c=哔)\\d+/; console.log(reg2.exec(str)); // 120 dotAll模式\n正则的dot(.)匹配除换行符以外的任意一个字符，dotAll模式下dot(.)将可以匹配任意字符（包括换行符）。 // 正则表达式的最后加了s符号 const reg = /.*/s; ES10 对象扩展方法Object.fromEntries 接收一个二维数组或Map将其转换为对象，与ES8的Object.entries方法互为逆运算\nconst res = Object.fromEntries([ ['name', 'horizon'] ]) console.log(res); // {name: 'horizon'} const m = new Map(); m.set('name', 'horizon'); const obj = Object.fromEntries(m); console.log(obj); // {name: 'horizon'} 字符串扩展方法trimStart与trimEnd trimStart：清除字符串左侧空白字符 trimEnd：清除字符串右侧空白字符 数组扩展方法flat与flatMap flat：将多维数组转化为低维数组，参数可传入需要降几维，默认值为1 flatMap：将map的结果进行维度降低 const array = [1,2,[3,4,[5,6]]]; console.log(array.flat()); // [1,2,3,4,[5,6]] console.log(array.flat(2)); // [1,2,3,4,5,6] const arr = [1,2,3,4,5]; const newArr = arr.flatMap(each =\u003e [each * 10]); // [10,20,30,40,50] Symbol.prototype.description const s = Symbol('horizon'); console.log(Symbol.description); // horizon ES11 私有属性 class Person { // 公有属性 name; // 私有属性 #age; constructor(name, age) { this.name = name; this.#age = age; } } const person = new Person('horizon', 23); console.log(person.name); // horizon console.log(person.#age); // error Promise.allSettled 接收一个Promise数组，不管Promise数组内的Promise是否成功或失败，allSettled返回的Promise都是成功状态的，这与all不同。\n字符串扩展String.matchAll 将所有的匹配结果取出来，返回一个可迭代的对象，可用for…of循环遍历\n可选链操作符（?.） function main(config) { // const host = config \u0026\u0026 config.db \u0026\u0026 config.db.host; const host = config?.db?.host; } const config = { db: { host: '127.0.0.1' } } 动态import 使用 import(’/path’) 动态导入模块，返回一个Promise对象\nBigInt类型 const num1 = 123n; const num2 = BigInt(123); 绝对全局对象globalThis ","description":"","tags":["ES6","ES新特性"],"title":"ES6-11新特性学习笔记（B站）","uri":"/posts/2021-04-18-atguigu_es_feature_note/"},{"categories":["静态网站"],"content":"下载Hugo 二进制安装 1. 上github（https://github.com/gohugoio/hugo/releases）下载对应的压缩包，然后进行解压，再将目录配置到环境变量中 2. 执行：hugo version，验证是否安装成功 源码安装 1. 安装Git（https://git-scm.com/downloads） 2. 安装Golang（https://golang.org/dl/） 3. 安装Hugo git clone https://github.com/gohugoio/hugo.git cd hugo go install PS：如果需要下载支持编译SASS的版本，需要为clone时加上--tags extended参数， 安装go模块由go install改为执行：CGO_ENABLED=1 go install --tags extended， 如果安装go模块时报错：exec: \"gcc\": executable file not found in %PATH%，可按参考文献1做法安装gcc后再重试 PS：如果按源码的方式安装，后续执行hugo命令可能需要在git的客户端内才能执行，在windows cmd客户端内执行可能会报一下错误 创建项目 # 生成项目 hugo new site \"项目名\" # 生成博文 hogo new posts/xxx.md 安装主题 官网主题库：Hugo Themes\n# 引入主题 git submodule add \u003c主题仓库地址\u003e themes/\u003c主题名\u003e 修改配置 一般主题都有提供一份config.toml或config.yml配置，将其拷贝到项目的根目录并进行修改即可，如果使用Github Pages发布的话，需要在config.toml内添加：publishDir = docs 配置\n生成静态站点的目录 # 根目录下执行hugo命令生成静态站点的目录，默认为 public 目录 hugo 配置Github Pages 在仓库的Settings -\u003e Options下方找到Github Pages配置，配置source为以上生成的博客文件夹 参考文献 golang 编译cgo模块exec: “gcc”: executable file not found in %PATH% Hugo 和 GitHub Pages 使用指南 浅谈我为什么从 HEXO 迁移到 HUGO ","description":"","tags":["Hugo","搭建个人博客"],"title":"Hugo + Github Pages搭建个人博客","uri":"/posts/2021-03-03-hugo_start/"}]